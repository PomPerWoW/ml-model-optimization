{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import pickle\n",
                "from app.util.timer import Timer\n",
                "from app.util.Differ import Differ\n",
                "from main import YoloRuntimeTest"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "CPU input"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Initilize YOLO runtime test class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "yolo_runtime_test = YoloRuntimeTest()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 138.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 138.2ms\n",
                        "Speed: 3.0ms preprocess, 138.2ms inference, 489.1ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 3.0ms preprocess, 138.2ms inference, 489.1ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 2.1936 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9335812926292419, 859, 1689, 1509, 2235], ['car', 0.9225443601608276, 155, 1659, 620, 2021], ['traffic light', 0.9069226384162903, 366, 761, 480, 984], ['traffic light', 0.8241053819656372, 0, 769, 45, 995], ['traffic light', 0.7964197397232056, 1405, 1395, 1452, 1493], ['traffic light', 0.7787498831748962, 1181, 1379, 1226, 1475], ['traffic light', 0.7608105540275574, 991, 1367, 1041, 1467], ['car', 0.7513810396194458, 1981, 1750, 2304, 1865], ['car', 0.7438511848449707, 2340, 1729, 2527, 1813], ['car', 0.7359843254089355, 3643, 1690, 3978, 1810], ['traffic light', 0.7012598514556885, 1987, 1117, 2038, 1261]], 2.193593000000001)\n"
                    ]
                }
            ],
            "source": [
                "cpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
                "print(cpu_pytorch_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 121.9ms\n",
                        "Speed: 3.0ms preprocess, 121.9ms inference, 425.6ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 2.3182 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9310466051101685, 859, 1690, 1509, 2235], ['car', 0.921970009803772, 155, 1659, 620, 2021], ['traffic light', 0.9051550030708313, 366, 761, 480, 984], ['traffic light', 0.8343842625617981, 0, 769, 45, 995], ['traffic light', 0.7965543270111084, 1405, 1395, 1452, 1494], ['traffic light', 0.7801229953765869, 1181, 1379, 1227, 1475], ['traffic light', 0.7694882750511169, 991, 1367, 1041, 1467], ['car', 0.7377140522003174, 2339, 1730, 2529, 1814], ['car', 0.733312726020813, 3642, 1690, 3978, 1809], ['car', 0.7331985831260681, 1981, 1750, 2305, 1864], ['traffic light', 0.6988853216171265, 1987, 1117, 2038, 1261]], 2.3182441999999988)\n"
                    ]
                }
            ],
            "source": [
                "cpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
                "print(cpu_openvino_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 182.3ms\n",
                        "Speed: 4.5ms preprocess, 182.3ms inference, 578.6ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 2.3534 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9310464262962341, 859, 1690, 1509, 2235], ['car', 0.9219695925712585, 155, 1659, 620, 2021], ['traffic light', 0.9051545858383179, 366, 761, 480, 984], ['traffic light', 0.8343836665153503, 0, 769, 45, 995], ['traffic light', 0.796554684638977, 1405, 1395, 1452, 1494], ['traffic light', 0.7801216840744019, 1181, 1379, 1227, 1475], ['traffic light', 0.7694879770278931, 991, 1367, 1041, 1467], ['car', 0.7377135753631592, 2339, 1730, 2529, 1814], ['car', 0.7333121299743652, 3642, 1690, 3978, 1809], ['car', 0.7331987619400024, 1981, 1750, 2305, 1864], ['traffic light', 0.6988840103149414, 1987, 1117, 2038, 1261]], 2.3533709000000016)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
                "print(cpu_onnx_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1915 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "([['car', 0.9386053085327148, 856, 1686, 1511, 2235], ['traffic light', 0.9161757230758667, 366, 761, 480, 985], ['car', 0.9144351482391357, 154, 1658, 620, 2019], ['traffic light', 0.8307116031646729, 0, 768, 44, 997], ['traffic light', 0.8175124526023865, 991, 1367, 1042, 1466], ['traffic light', 0.8093212842941284, 1403, 1394, 1452, 1491], ['traffic light', 0.7603057622909546, 1180, 1380, 1226, 1475], ['car', 0.7564687728881836, 2338, 1730, 2529, 1814], ['traffic light', 0.6475574970245361, 1985, 1115, 2039, 1261], ['car', 0.6093098521232605, 3640, 1692, 3969, 1803]], 0.19148239999999994)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
                "print(cpu_onnx_runtime_image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Difference CPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.set_option('display.expand_frame_repr', False)\n",
                "\n",
                "def generate_difference_df(image1, image2, label):\n",
                "    differ = Differ(np.array(image1), np.array(image2))\n",
                "    result = differ.find_difference()\n",
                "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff (px)\"])\n",
                "\n",
                "df_pt_openvino = generate_difference_df(cpu_pytorch_image[0], cpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
                "df_pt_onnx = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
                "df_pt_onnxruntime = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
                "\n",
                "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
                "\n",
                "print(df_combined)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Average CPU Time (10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "result_time = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 448x640 1 person, 1 bicycle, 3 cars, 3 trucks, 134.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 448x640 1 person, 1 bicycle, 3 cars, 3 trucks, 134.3ms\n",
                        "Speed: 2.0ms preprocess, 134.3ms inference, 402.1ms postprocess per image at shape (1, 3, 448, 640)\n",
                        "Speed: 2.0ms preprocess, 134.3ms inference, 402.1ms postprocess per image at shape (1, 3, 448, 640)\n",
                        "Elapsed time: 2.0411 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
                        "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 6 cars, 2 traffic lights, 129.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 6 cars, 2 traffic lights, 129.1ms\n",
                        "Speed: 1.5ms preprocess, 129.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 1.5ms preprocess, 129.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6383 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 444, 102, 578]\n",
                        "Class: car, Confidence: 0.81, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.62, Box: [237, 1072, 323, 1114]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 129.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 129.2ms\n",
                        "Speed: 2.0ms preprocess, 129.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 129.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6839 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 1 person, 4 cars, 6 traffic lights, 129.7ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 1 person, 4 cars, 6 traffic lights, 129.7ms\n",
                        "Speed: 3.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 3.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6839 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 664, 2355]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1263, 1408, 1428]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1907, 1279, 1966, 1447]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2324]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.79, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.66, Box: [411, 1513, 488, 1639]\n",
                        "Class: car, Confidence: 0.64, Box: [2998, 2193, 3468, 2359]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 480x640 5 cars, 6 traffic lights, 128.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 480x640 5 cars, 6 traffic lights, 128.2ms\n",
                        "Speed: 2.0ms preprocess, 128.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 128.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6797 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 384x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 119.7ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 384x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 119.7ms\n",
                        "Speed: 2.0ms preprocess, 119.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
                        "Speed: 2.0ms preprocess, 119.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
                        "Elapsed time: 1.6502 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [110, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3463, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.70, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.65, Box: [2714, 1234, 2823, 1272]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 480x640 3 cars, 118.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 480x640 3 cars, 118.6ms\n",
                        "Speed: 2.5ms preprocess, 118.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.5ms preprocess, 118.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6435 seconds\n",
                        "Class: car, Confidence: 0.90, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.62, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 480x640 1 car, 2 traffic lights, 120.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 480x640 1 car, 2 traffic lights, 120.1ms\n",
                        "Speed: 2.5ms preprocess, 120.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.5ms preprocess, 120.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6801 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1305, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.74, Box: [2448, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.62, Box: [2251, 1266, 2273, 1335]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 480x640 6 cars, 117.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 480x640 6 cars, 117.6ms\n",
                        "Speed: 2.0ms preprocess, 117.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 117.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6491 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2611, 1350, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.85, Box: [1688, 1325, 1956, 1541]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1734, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 480x640 (no detections), 120.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 480x640 (no detections), 120.2ms\n",
                        "Speed: 2.5ms preprocess, 120.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.5ms preprocess, 120.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 1.6871 seconds\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 3 cars, 3 trucks, 125.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 3 cars, 3 trucks, 125.2ms\n",
                        "Speed: 2.0ms preprocess, 125.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 125.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7570 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
                        "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 5 cars, 2 traffic lights, 126.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 5 cars, 2 traffic lights, 126.2ms\n",
                        "Speed: 2.5ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7122 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 442, 102, 578]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: car, Confidence: 0.80, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 127.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 127.3ms\n",
                        "Speed: 2.0ms preprocess, 127.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 127.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7595 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 1 person, 4 cars, 6 traffic lights, 123.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 1 person, 4 cars, 6 traffic lights, 123.6ms\n",
                        "Speed: 2.0ms preprocess, 123.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 123.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7574 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
                        "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 5 cars, 6 traffic lights, 121.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 5 cars, 6 traffic lights, 121.2ms\n",
                        "Speed: 2.5ms preprocess, 121.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 121.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7421 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 126.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 126.6ms\n",
                        "Speed: 1.0ms preprocess, 126.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.0ms preprocess, 126.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7478 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [109, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3464, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.68, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.66, Box: [2714, 1234, 2823, 1272]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 130.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 130.2ms\n",
                        "Speed: 2.0ms preprocess, 130.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 130.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7126 seconds\n",
                        "Class: car, Confidence: 0.89, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.66, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 2 traffic lights, 123.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 2 traffic lights, 123.9ms\n",
                        "Speed: 2.5ms preprocess, 123.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 123.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7526 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1304, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.73, Box: [2447, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [2251, 1266, 2273, 1335]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 6 cars, 127.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 6 cars, 127.3ms\n",
                        "Speed: 2.5ms preprocess, 127.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 127.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7381 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2612, 1349, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.84, Box: [1688, 1325, 1956, 1542]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1733, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 (no detections), 122.0ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 (no detections), 122.0ms\n",
                        "Speed: 2.6ms preprocess, 122.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.6ms preprocess, 122.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7415 seconds\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 3 cars, 3 trucks, 159.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 3 cars, 3 trucks, 159.6ms\n",
                        "Speed: 2.0ms preprocess, 159.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 159.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7349 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
                        "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 5 cars, 2 traffic lights, 145.7ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 5 cars, 2 traffic lights, 145.7ms\n",
                        "Speed: 1.5ms preprocess, 145.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.5ms preprocess, 145.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7253 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 442, 102, 578]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: car, Confidence: 0.80, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 136.4ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 136.4ms\n",
                        "Speed: 3.5ms preprocess, 136.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.5ms preprocess, 136.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7643 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 1 person, 4 cars, 6 traffic lights, 175.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 1 person, 4 cars, 6 traffic lights, 175.6ms\n",
                        "Speed: 3.0ms preprocess, 175.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 175.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7930 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
                        "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 5 cars, 6 traffic lights, 143.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 5 cars, 6 traffic lights, 143.3ms\n",
                        "Speed: 3.0ms preprocess, 143.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 143.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7682 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 151.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 1 person, 4 cars, 1 motorcycle, 1 umbrella, 151.6ms\n",
                        "Speed: 2.0ms preprocess, 151.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 151.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7490 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [109, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3464, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.68, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.66, Box: [2714, 1234, 2823, 1272]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 143.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 143.3ms\n",
                        "Speed: 3.0ms preprocess, 143.3ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 143.3ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7441 seconds\n",
                        "Class: car, Confidence: 0.89, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.66, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 2 traffic lights, 173.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 2 traffic lights, 173.3ms\n",
                        "Speed: 3.0ms preprocess, 173.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 173.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7893 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1304, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.73, Box: [2447, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [2251, 1266, 2273, 1335]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 6 cars, 151.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 6 cars, 151.6ms\n",
                        "Speed: 2.0ms preprocess, 151.6ms inference, 10.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 151.6ms inference, 10.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7666 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2612, 1349, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.84, Box: [1688, 1325, 1956, 1542]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1733, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 (no detections), 122.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 (no detections), 122.2ms\n",
                        "Speed: 3.0ms preprocess, 122.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 122.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.9531 seconds\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1390 seconds\n",
                        "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
                        "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
                        "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
                        "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
                        "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
                        "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1470 seconds\n",
                        "Class: traffic light, Confidence: 0.83, Box: [36, 448, 102, 579]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [513, 465, 564, 589]\n",
                        "Class: car, Confidence: 0.82, Box: [802, 1050, 899, 1115]\n",
                        "Class: car, Confidence: 0.81, Box: [970, 1034, 1066, 1117]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [955, 515, 1045, 600]\n",
                        "Class: car, Confidence: 0.66, Box: [924, 1051, 958, 1081]\n",
                        "Class: truck, Confidence: 0.65, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.63, Box: [381, 1058, 443, 1113]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1860 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1902 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [0, 2016, 659, 2356]\n",
                        "Class: car, Confidence: 0.90, Box: [815, 2100, 1316, 2297]\n",
                        "Class: person, Confidence: 0.84, Box: [1488, 2047, 1568, 2320]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [1346, 1261, 1408, 1429]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [1907, 1281, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1159, 1203, 1227, 1368]\n",
                        "Class: car, Confidence: 0.78, Box: [1795, 2214, 1917, 2288]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [3015, 773, 3161, 1041]\n",
                        "Class: traffic light, Confidence: 0.71, Box: [628, 1188, 699, 1355]\n",
                        "Class: traffic light, Confidence: 0.68, Box: [539, 1578, 602, 1692]\n",
                        "Class: traffic light, Confidence: 0.68, Box: [2751, 1027, 2872, 1242]\n",
                        "Class: car, Confidence: 0.66, Box: [3643, 2222, 4029, 2373]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [411, 1513, 487, 1630]\n",
                        "Class: person, Confidence: 0.61, Box: [2209, 2169, 2266, 2290]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1790 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1659 seconds\n",
                        "Class: car, Confidence: 0.95, Box: [108, 1117, 1609, 1904]\n",
                        "Class: car, Confidence: 0.93, Box: [3464, 1269, 3837, 1596]\n",
                        "Class: car, Confidence: 0.84, Box: [1599, 1224, 1817, 1411]\n",
                        "Class: person, Confidence: 0.71, Box: [2727, 1256, 2806, 1381]\n",
                        "Class: umbrella, Confidence: 0.70, Box: [2713, 1235, 2824, 1267]\n",
                        "Class: person, Confidence: 0.68, Box: [1974, 1174, 2164, 1495]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1492 seconds\n",
                        "Class: car, Confidence: 0.84, Box: [1283, 865, 1423, 981]\n",
                        "Class: car, Confidence: 0.78, Box: [1179, 863, 1250, 923]\n",
                        "Class: car, Confidence: 0.74, Box: [846, 872, 895, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1830 seconds\n",
                        "Class: car, Confidence: 0.92, Box: [1302, 1641, 1767, 2005]\n",
                        "Class: traffic light, Confidence: 0.71, Box: [2446, 1261, 2471, 1317]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1634 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [2608, 1350, 3261, 1731]\n",
                        "Class: car, Confidence: 0.92, Box: [1785, 1360, 2131, 1612]\n",
                        "Class: car, Confidence: 0.87, Box: [1686, 1327, 1956, 1542]\n",
                        "Class: car, Confidence: 0.84, Box: [1056, 1346, 1241, 1482]\n",
                        "Class: car, Confidence: 0.75, Box: [1617, 1365, 1735, 1471]\n",
                        "Class: car, Confidence: 0.66, Box: [1568, 1346, 1673, 1454]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1808 seconds\n",
                        "Class: car, Confidence: 0.60, Box: [1914, 1936, 1989, 1994]\n"
                    ]
                }
            ],
            "source": [
                "def run_inference_with_args(inference_func, args):\n",
                "    def wrapper():\n",
                "        return inference_func(args)\n",
                "    return wrapper\n",
                "\n",
                "def collect_execution_times(run_inference_func, args, iterations=10):\n",
                "    execution_times = []\n",
                "    for i in range(iterations):\n",
                "        args[\"source\"] = f\"./app/assets/sample_image_{i}.jpg\"\n",
                "        wrapper_func = run_inference_with_args(run_inference_func, args)\n",
                "        execution_time = wrapper_func()\n",
                "        execution_times.append(execution_time[1] * 100)\n",
                "    return execution_times\n",
                "\n",
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\",\n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_pytorch))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_openvino))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_onnx))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.onnxruntime_run_image, args_onnx))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>pytorch time cpu (ms)</th>\n",
                            "      <th>openvino+ultralytics time cpu (ms)</th>\n",
                            "      <th>onnx​+ultralytics time cpu (ms)</th>\n",
                            "      <th>onnx runtime time cpu (ms)</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>count</th>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>mean</th>\n",
                            "      <td>170.367616</td>\n",
                            "      <td>174.209288</td>\n",
                            "      <td>177.879698</td>\n",
                            "      <td>16.834223</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>std</th>\n",
                            "      <td>12.006981</td>\n",
                            "      <td>1.726269</td>\n",
                            "      <td>6.500691</td>\n",
                            "      <td>1.819723</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>min</th>\n",
                            "      <td>163.831230</td>\n",
                            "      <td>171.217580</td>\n",
                            "      <td>172.533300</td>\n",
                            "      <td>13.899370</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>50%</th>\n",
                            "      <td>167.985510</td>\n",
                            "      <td>174.496640</td>\n",
                            "      <td>176.547890</td>\n",
                            "      <td>17.242400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>90%</th>\n",
                            "      <td>172.253711</td>\n",
                            "      <td>175.760311</td>\n",
                            "      <td>180.904234</td>\n",
                            "      <td>18.642716</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>95%</th>\n",
                            "      <td>188.179710</td>\n",
                            "      <td>175.855715</td>\n",
                            "      <td>188.106817</td>\n",
                            "      <td>18.829358</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>max</th>\n",
                            "      <td>204.105710</td>\n",
                            "      <td>175.951120</td>\n",
                            "      <td>195.309400</td>\n",
                            "      <td>19.016000</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       pytorch time cpu (ms)  openvino+ultralytics time cpu (ms)  \\\n",
                            "count              10.000000                           10.000000   \n",
                            "mean              170.367616                          174.209288   \n",
                            "std                12.006981                            1.726269   \n",
                            "min               163.831230                          171.217580   \n",
                            "50%               167.985510                          174.496640   \n",
                            "90%               172.253711                          175.760311   \n",
                            "95%               188.179710                          175.855715   \n",
                            "max               204.105710                          175.951120   \n",
                            "\n",
                            "       onnx​+ultralytics time cpu (ms)  onnx runtime time cpu (ms)  \n",
                            "count                        10.000000                   10.000000  \n",
                            "mean                        177.879698                   16.834223  \n",
                            "std                           6.500691                    1.819723  \n",
                            "min                         172.533300                   13.899370  \n",
                            "50%                         176.547890                   17.242400  \n",
                            "90%                         180.904234                   18.642716  \n",
                            "95%                         188.106817                   18.829358  \n",
                            "max                         195.309400                   19.016000  "
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "result_time = np.array(result_time)\n",
                "df = pd.DataFrame(np.transpose(result_time), \n",
                "                  columns=[\"pytorch time cpu (ms)\",\n",
                "                           \"openvino+ultralytics time cpu (ms)\",\n",
                "                           \"onnx​+ultralytics time cpu (ms)\", \n",
                "                           \"onnx runtime time cpu (ms)\"])\n",
                "df.describe(percentiles=[.9, .95])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save CPU result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('./app/saved_pkl/cpu_df.pkl', 'wb') as f:\n",
                "    pickle.dump(df, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "yolov9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import pickle\n",
                "from app.util.timer import Timer\n",
                "from app.util.Differ import Differ\n",
                "from main import YoloRuntimeTest\n",
                "from functools import partial"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "CPU input"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_1.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_1.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_1.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Initilize YOLO runtime test class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "yolo_runtime_test = YoloRuntimeTest()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "infer time: 0.13304789999995137 s\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 9 cars, 5 traffic lights, 132.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 9 cars, 5 traffic lights, 132.9ms\n",
                        "Speed: 2.0ms preprocess, 132.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 132.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1329 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 444, 102, 578]\n",
                        "Class: car, Confidence: 0.81, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.62, Box: [237, 1072, 323, 1114]\n",
                        "Class: car, Confidence: 0.59, Box: [691, 1050, 731, 1083]\n",
                        "Class: traffic light, Confidence: 0.50, Box: [954, 507, 1047, 599]\n",
                        "Class: car, Confidence: 0.45, Box: [736, 1049, 779, 1080]\n",
                        "Class: car, Confidence: 0.44, Box: [1207, 1023, 1274, 1051]\n",
                        "Class: traffic light, Confidence: 0.34, Box: [956, 518, 1001, 598]\n",
                        "Class: traffic light, Confidence: 0.21, Box: [1006, 515, 1046, 598]\n",
                        "([['car', 0.8548609018325806, 970, 1035, 1066, 1119], ['car', 0.8419999480247498, 803, 1049, 897, 1116], ['traffic light', 0.8402162194252014, 514, 463, 565, 588], ['traffic light', 0.8352504968643188, 36, 444, 102, 578], ['car', 0.8138196468353271, 424, 1015, 708, 1217], ['car', 0.7132752537727356, 381, 1064, 441, 1114], ['car', 0.6835986375808716, 923, 1049, 958, 1081], ['car', 0.6163407564163208, 237, 1072, 323, 1114], ['car', 0.588860273361206, 691, 1050, 731, 1083], ['traffic light', 0.4954128563404083, 954, 507, 1047, 599], ['car', 0.4466933012008667, 736, 1049, 779, 1080], ['car', 0.44175779819488525, 1207, 1023, 1274, 1051], ['traffic light', 0.34057843685150146, 956, 518, 1001, 598], ['traffic light', 0.21309614181518555, 1006, 515, 1046, 598]], 0.13293719291687012)\n"
                    ]
                }
            ],
            "source": [
                "cpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
                "print(cpu_pytorch_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
                "print(cpu_openvino_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "infer time: 0.12225300000000061 s\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 129.0ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 129.0ms\n",
                        "Speed: 1.5ms preprocess, 129.0ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.5ms preprocess, 129.0ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1290 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 442, 102, 578]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: car, Confidence: 0.80, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.60, Box: [237, 1072, 323, 1113]\n",
                        "Class: car, Confidence: 0.59, Box: [691, 1050, 731, 1083]\n",
                        "Class: traffic light, Confidence: 0.47, Box: [954, 508, 1047, 599]\n",
                        "Class: car, Confidence: 0.44, Box: [736, 1049, 779, 1081]\n",
                        "Class: car, Confidence: 0.42, Box: [1207, 1023, 1274, 1051]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [956, 518, 1001, 598]\n",
                        "Class: traffic light, Confidence: 0.25, Box: [1006, 515, 1046, 598]\n",
                        "([['car', 0.8504844903945923, 970, 1035, 1066, 1119], ['traffic light', 0.8410413265228271, 514, 463, 565, 588], ['traffic light', 0.8378149271011353, 36, 442, 102, 578], ['car', 0.8354570865631104, 803, 1049, 897, 1116], ['car', 0.8014183044433594, 424, 1015, 708, 1217], ['car', 0.7118813991546631, 381, 1064, 441, 1114], ['car', 0.6803537011146545, 923, 1049, 958, 1081], ['car', 0.5984785556793213, 237, 1072, 323, 1113], ['car', 0.591949462890625, 691, 1050, 731, 1083], ['traffic light', 0.4738231897354126, 954, 508, 1047, 599], ['car', 0.443163126707077, 736, 1049, 779, 1081], ['car', 0.42404744029045105, 1207, 1023, 1274, 1051], ['traffic light', 0.3874286711215973, 956, 518, 1001, 598], ['traffic light', 0.247898131608963, 1006, 515, 1046, 598]], 0.12904644012451172)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
                "print(cpu_onnx_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
                "print(cpu_onnx_runtime_image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Difference CPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.set_option('display.expand_frame_repr', False)\n",
                "\n",
                "def generate_difference_df(image1, image2, label):\n",
                "    differ = Differ(np.array(image1), np.array(image2))\n",
                "    result = differ.find_difference()\n",
                "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff (px)\"])\n",
                "\n",
                "df_pt_openvino = generate_difference_df(cpu_pytorch_image[0], cpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
                "df_pt_onnx = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
                "df_pt_onnxruntime = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
                "\n",
                "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
                "\n",
                "print(df_combined)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Average CPU Time (10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "result_time = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 146.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 146.9ms\n",
                        "Speed: 2.0ms preprocess, 146.9ms inference, 495.7ms postprocess per image at shape (1, 3, 448, 640)\n",
                        "Speed: 2.0ms preprocess, 146.9ms inference, 495.7ms postprocess per image at shape (1, 3, 448, 640)\n",
                        "Elapsed time: 0.1469 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
                        "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
                        "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
                        "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 9 cars, 5 traffic lights, 131.5ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 480x640 9 cars, 5 traffic lights, 131.5ms\n",
                        "Speed: 1.0ms preprocess, 131.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 1.0ms preprocess, 131.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1315 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 444, 102, 578]\n",
                        "Class: car, Confidence: 0.81, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.62, Box: [237, 1072, 323, 1114]\n",
                        "Class: car, Confidence: 0.59, Box: [691, 1050, 731, 1083]\n",
                        "Class: traffic light, Confidence: 0.50, Box: [954, 507, 1047, 599]\n",
                        "Class: car, Confidence: 0.45, Box: [736, 1049, 779, 1080]\n",
                        "Class: car, Confidence: 0.44, Box: [1207, 1023, 1274, 1051]\n",
                        "Class: traffic light, Confidence: 0.34, Box: [956, 518, 1001, 598]\n",
                        "Class: traffic light, Confidence: 0.21, Box: [1006, 515, 1046, 598]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 7 cars, 9 traffic lights, 125.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 7 cars, 9 traffic lights, 125.1ms\n",
                        "Speed: 2.0ms preprocess, 125.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 125.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1251 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1928, 1156, 1974, 1289]\n",
                        "Class: traffic light, Confidence: 0.46, Box: [1887, 1190, 1925, 1301]\n",
                        "Class: car, Confidence: 0.41, Box: [859, 1783, 941, 1877]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [2294, 1465, 2402, 1547]\n",
                        "Class: car, Confidence: 0.21, Box: [3938, 1727, 3982, 1809]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 6 persons, 8 cars, 1 truck, 9 traffic lights, 120.8ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 6 persons, 8 cars, 1 truck, 9 traffic lights, 120.8ms\n",
                        "Speed: 1.0ms preprocess, 120.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 1.0ms preprocess, 120.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1208 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 664, 2355]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1263, 1408, 1428]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1907, 1279, 1966, 1447]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2324]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.79, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.66, Box: [411, 1513, 488, 1639]\n",
                        "Class: car, Confidence: 0.64, Box: [2998, 2193, 3468, 2359]\n",
                        "Class: traffic light, Confidence: 0.57, Box: [2753, 1024, 2891, 1246]\n",
                        "Class: car, Confidence: 0.51, Box: [3646, 2242, 3771, 2366]\n",
                        "Class: truck, Confidence: 0.45, Box: [2994, 2192, 3465, 2360]\n",
                        "Class: person, Confidence: 0.44, Box: [2219, 2174, 2260, 2291]\n",
                        "Class: person, Confidence: 0.43, Box: [2002, 2151, 2063, 2300]\n",
                        "Class: car, Confidence: 0.43, Box: [1382, 2157, 1516, 2258]\n",
                        "Class: car, Confidence: 0.40, Box: [1300, 2156, 1402, 2255]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [3474, 1415, 3714, 1612]\n",
                        "Class: person, Confidence: 0.39, Box: [2649, 2170, 2705, 2315]\n",
                        "Class: person, Confidence: 0.37, Box: [2304, 2142, 2352, 2297]\n",
                        "Class: car, Confidence: 0.29, Box: [1301, 2154, 1522, 2260]\n",
                        "Class: traffic light, Confidence: 0.28, Box: [540, 1570, 606, 1698]\n",
                        "Class: person, Confidence: 0.27, Box: [2160, 2167, 2206, 2291]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 480x640 7 cars, 9 traffic lights, 119.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 480x640 7 cars, 9 traffic lights, 119.3ms\n",
                        "Speed: 2.5ms preprocess, 119.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.5ms preprocess, 119.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1193 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1928, 1156, 1974, 1289]\n",
                        "Class: traffic light, Confidence: 0.46, Box: [1887, 1190, 1925, 1301]\n",
                        "Class: car, Confidence: 0.41, Box: [859, 1783, 941, 1877]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [2294, 1465, 2402, 1547]\n",
                        "Class: car, Confidence: 0.21, Box: [3938, 1727, 3982, 1809]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 384x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 111.8ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 384x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 111.8ms\n",
                        "Speed: 2.5ms preprocess, 111.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
                        "Speed: 2.5ms preprocess, 111.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
                        "Elapsed time: 0.1118 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [110, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3463, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.70, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.65, Box: [2714, 1234, 2823, 1272]\n",
                        "Class: person, Confidence: 0.43, Box: [2741, 1269, 2800, 1382]\n",
                        "Class: car, Confidence: 0.27, Box: [1320, 1149, 1564, 1359]\n",
                        "Class: motorcycle, Confidence: 0.23, Box: [2130, 1238, 2211, 1452]\n",
                        "Class: person, Confidence: 0.21, Box: [2131, 1214, 2214, 1389]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 480x640 3 cars, 121.4ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 480x640 3 cars, 121.4ms\n",
                        "Speed: 2.0ms preprocess, 121.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 121.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1214 seconds\n",
                        "Class: car, Confidence: 0.90, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.62, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 480x640 2 cars, 1 truck, 3 traffic lights, 121.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 480x640 2 cars, 1 truck, 3 traffic lights, 121.9ms\n",
                        "Speed: 1.0ms preprocess, 121.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 1.0ms preprocess, 121.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1219 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1305, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.74, Box: [2448, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.62, Box: [2251, 1266, 2273, 1335]\n",
                        "Class: truck, Confidence: 0.52, Box: [2006, 1592, 2212, 1743]\n",
                        "Class: car, Confidence: 0.40, Box: [2006, 1592, 2212, 1743]\n",
                        "Class: traffic light, Confidence: 0.35, Box: [1100, 1295, 1131, 1350]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 480x640 14 cars, 1 stop sign, 116.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 480x640 14 cars, 1 stop sign, 116.9ms\n",
                        "Speed: 1.0ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 1.0ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1169 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2611, 1350, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.85, Box: [1688, 1325, 1956, 1541]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1734, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "Class: car, Confidence: 0.59, Box: [1212, 1347, 1282, 1456]\n",
                        "Class: stop sign, Confidence: 0.57, Box: [2553, 1144, 2611, 1270]\n",
                        "Class: car, Confidence: 0.51, Box: [1346, 1349, 1403, 1399]\n",
                        "Class: car, Confidence: 0.50, Box: [1480, 1360, 1532, 1405]\n",
                        "Class: car, Confidence: 0.49, Box: [1242, 1342, 1322, 1432]\n",
                        "Class: car, Confidence: 0.35, Box: [1517, 1361, 1581, 1416]\n",
                        "Class: car, Confidence: 0.33, Box: [1530, 1359, 1594, 1425]\n",
                        "Class: car, Confidence: 0.29, Box: [1515, 1342, 1615, 1421]\n",
                        "Class: car, Confidence: 0.24, Box: [1286, 1346, 1334, 1421]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 480x640 3 cars, 119.5ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 480x640 3 cars, 119.5ms\n",
                        "Speed: 2.0ms preprocess, 119.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 119.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 0.1195 seconds\n",
                        "Class: car, Confidence: 0.54, Box: [1915, 1937, 1988, 1995]\n",
                        "Class: car, Confidence: 0.33, Box: [2418, 1906, 2508, 1944]\n",
                        "Class: car, Confidence: 0.28, Box: [2148, 1930, 2231, 1989]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 120.8ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 120.8ms\n",
                        "Speed: 2.0ms preprocess, 120.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 120.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1208 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
                        "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
                        "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
                        "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
                        "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 124.4ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 124.4ms\n",
                        "Speed: 1.5ms preprocess, 124.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.5ms preprocess, 124.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1244 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 442, 102, 578]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: car, Confidence: 0.80, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.60, Box: [237, 1072, 323, 1113]\n",
                        "Class: car, Confidence: 0.59, Box: [691, 1050, 731, 1083]\n",
                        "Class: traffic light, Confidence: 0.47, Box: [954, 508, 1047, 599]\n",
                        "Class: car, Confidence: 0.44, Box: [736, 1049, 779, 1081]\n",
                        "Class: car, Confidence: 0.42, Box: [1207, 1023, 1274, 1051]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [956, 518, 1001, 598]\n",
                        "Class: traffic light, Confidence: 0.25, Box: [1006, 515, 1046, 598]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 6 cars, 9 traffic lights, 124.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 6 cars, 9 traffic lights, 124.1ms\n",
                        "Speed: 1.0ms preprocess, 124.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.0ms preprocess, 124.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1241 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1927, 1156, 1974, 1290]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [2294, 1464, 2402, 1547]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1887, 1188, 1924, 1304]\n",
                        "Class: car, Confidence: 0.40, Box: [859, 1783, 941, 1877]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 122.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 122.1ms\n",
                        "Speed: 2.0ms preprocess, 122.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 122.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1221 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
                        "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
                        "Class: traffic light, Confidence: 0.55, Box: [2753, 1024, 2889, 1246]\n",
                        "Class: car, Confidence: 0.52, Box: [3646, 2242, 3771, 2365]\n",
                        "Class: car, Confidence: 0.46, Box: [1382, 2156, 1514, 2258]\n",
                        "Class: person, Confidence: 0.45, Box: [2219, 2174, 2261, 2290]\n",
                        "Class: car, Confidence: 0.44, Box: [1300, 2155, 1401, 2255]\n",
                        "Class: truck, Confidence: 0.43, Box: [2993, 2192, 3465, 2360]\n",
                        "Class: person, Confidence: 0.43, Box: [2003, 2150, 2063, 2300]\n",
                        "Class: person, Confidence: 0.39, Box: [2303, 2141, 2352, 2297]\n",
                        "Class: traffic light, Confidence: 0.38, Box: [3473, 1414, 3716, 1612]\n",
                        "Class: person, Confidence: 0.38, Box: [2649, 2169, 2706, 2315]\n",
                        "Class: car, Confidence: 0.28, Box: [1301, 2154, 1521, 2259]\n",
                        "Class: traffic light, Confidence: 0.28, Box: [540, 1571, 606, 1699]\n",
                        "Class: person, Confidence: 0.27, Box: [2160, 2168, 2206, 2291]\n",
                        "Class: traffic light, Confidence: 0.23, Box: [2963, 1034, 3178, 1224]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 6 cars, 9 traffic lights, 121.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 6 cars, 9 traffic lights, 121.3ms\n",
                        "Speed: 1.5ms preprocess, 121.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.5ms preprocess, 121.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1213 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1927, 1156, 1974, 1290]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [2294, 1464, 2402, 1547]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1887, 1188, 1924, 1304]\n",
                        "Class: car, Confidence: 0.40, Box: [859, 1783, 941, 1877]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 120.5ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 120.5ms\n",
                        "Speed: 2.0ms preprocess, 120.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 120.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1205 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [109, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3464, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.68, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.66, Box: [2714, 1234, 2823, 1272]\n",
                        "Class: person, Confidence: 0.43, Box: [2744, 1268, 2800, 1381]\n",
                        "Class: car, Confidence: 0.27, Box: [1320, 1149, 1564, 1359]\n",
                        "Class: motorcycle, Confidence: 0.23, Box: [2130, 1238, 2211, 1452]\n",
                        "Class: person, Confidence: 0.22, Box: [2131, 1214, 2214, 1387]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 128.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 128.6ms\n",
                        "Speed: 2.0ms preprocess, 128.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 128.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1286 seconds\n",
                        "Class: car, Confidence: 0.89, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.66, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 1 truck, 3 traffic lights, 123.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 1 truck, 3 traffic lights, 123.1ms\n",
                        "Speed: 1.5ms preprocess, 123.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 1.5ms preprocess, 123.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1231 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1304, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.73, Box: [2447, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [2251, 1266, 2273, 1335]\n",
                        "Class: truck, Confidence: 0.53, Box: [2006, 1592, 2212, 1743]\n",
                        "Class: traffic light, Confidence: 0.33, Box: [1099, 1295, 1132, 1350]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 14 cars, 1 stop sign, 122.5ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 14 cars, 1 stop sign, 122.5ms\n",
                        "Speed: 2.0ms preprocess, 122.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 122.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1225 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2612, 1349, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.84, Box: [1688, 1325, 1956, 1542]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1733, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "Class: stop sign, Confidence: 0.59, Box: [2553, 1144, 2611, 1269]\n",
                        "Class: car, Confidence: 0.58, Box: [1212, 1347, 1282, 1456]\n",
                        "Class: car, Confidence: 0.48, Box: [1346, 1350, 1403, 1399]\n",
                        "Class: car, Confidence: 0.48, Box: [1480, 1360, 1532, 1405]\n",
                        "Class: car, Confidence: 0.47, Box: [1245, 1344, 1319, 1433]\n",
                        "Class: car, Confidence: 0.35, Box: [1517, 1362, 1581, 1416]\n",
                        "Class: car, Confidence: 0.32, Box: [1530, 1360, 1592, 1425]\n",
                        "Class: car, Confidence: 0.26, Box: [1516, 1342, 1615, 1421]\n",
                        "Class: car, Confidence: 0.22, Box: [1295, 1351, 1335, 1420]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 3 cars, 121.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 3 cars, 121.3ms\n",
                        "Speed: 2.0ms preprocess, 121.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 121.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1213 seconds\n",
                        "Class: car, Confidence: 0.52, Box: [1915, 1936, 1988, 1995]\n",
                        "Class: car, Confidence: 0.36, Box: [2418, 1906, 2508, 1944]\n",
                        "Class: car, Confidence: 0.25, Box: [2148, 1930, 2231, 1989]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 130.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_0.jpg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 130.6ms\n",
                        "Speed: 2.0ms preprocess, 130.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 130.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1306 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
                        "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
                        "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
                        "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
                        "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
                        "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
                        "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
                        "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
                        "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
                        "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 125.7ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_1.jpg: 640x640 9 cars, 5 traffic lights, 125.7ms\n",
                        "Speed: 2.4ms preprocess, 125.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.4ms preprocess, 125.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1257 seconds\n",
                        "Class: car, Confidence: 0.85, Box: [970, 1035, 1066, 1119]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [514, 463, 565, 588]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [36, 442, 102, 578]\n",
                        "Class: car, Confidence: 0.84, Box: [803, 1049, 897, 1116]\n",
                        "Class: car, Confidence: 0.80, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.71, Box: [381, 1064, 441, 1114]\n",
                        "Class: car, Confidence: 0.68, Box: [923, 1049, 958, 1081]\n",
                        "Class: car, Confidence: 0.60, Box: [237, 1072, 323, 1113]\n",
                        "Class: car, Confidence: 0.59, Box: [691, 1050, 731, 1083]\n",
                        "Class: traffic light, Confidence: 0.47, Box: [954, 508, 1047, 599]\n",
                        "Class: car, Confidence: 0.44, Box: [736, 1049, 779, 1081]\n",
                        "Class: car, Confidence: 0.42, Box: [1207, 1023, 1274, 1051]\n",
                        "Class: traffic light, Confidence: 0.39, Box: [956, 518, 1001, 598]\n",
                        "Class: traffic light, Confidence: 0.25, Box: [1006, 515, 1046, 598]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 6 cars, 9 traffic lights, 182.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 6 cars, 9 traffic lights, 182.2ms\n",
                        "Speed: 2.0ms preprocess, 182.2ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 182.2ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1822 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1927, 1156, 1974, 1290]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [2294, 1464, 2402, 1547]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1887, 1188, 1924, 1304]\n",
                        "Class: car, Confidence: 0.40, Box: [859, 1783, 941, 1877]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 137.1ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 137.1ms\n",
                        "Speed: 2.8ms preprocess, 137.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.8ms preprocess, 137.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1371 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
                        "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
                        "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
                        "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
                        "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
                        "Class: traffic light, Confidence: 0.55, Box: [2753, 1024, 2889, 1246]\n",
                        "Class: car, Confidence: 0.52, Box: [3646, 2242, 3771, 2365]\n",
                        "Class: car, Confidence: 0.46, Box: [1382, 2156, 1514, 2258]\n",
                        "Class: person, Confidence: 0.45, Box: [2219, 2174, 2261, 2290]\n",
                        "Class: car, Confidence: 0.44, Box: [1300, 2155, 1401, 2255]\n",
                        "Class: truck, Confidence: 0.43, Box: [2993, 2192, 3465, 2360]\n",
                        "Class: person, Confidence: 0.43, Box: [2003, 2150, 2063, 2300]\n",
                        "Class: person, Confidence: 0.39, Box: [2303, 2141, 2352, 2297]\n",
                        "Class: traffic light, Confidence: 0.38, Box: [3473, 1414, 3716, 1612]\n",
                        "Class: person, Confidence: 0.38, Box: [2649, 2169, 2706, 2315]\n",
                        "Class: car, Confidence: 0.28, Box: [1301, 2154, 1521, 2259]\n",
                        "Class: traffic light, Confidence: 0.28, Box: [540, 1571, 606, 1699]\n",
                        "Class: person, Confidence: 0.27, Box: [2160, 2168, 2206, 2291]\n",
                        "Class: traffic light, Confidence: 0.23, Box: [2963, 1034, 3178, 1224]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 6 cars, 9 traffic lights, 145.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_4.jpg: 640x640 6 cars, 9 traffic lights, 145.3ms\n",
                        "Speed: 3.0ms preprocess, 145.3ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 145.3ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1453 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "Class: traffic light, Confidence: 0.53, Box: [1927, 1156, 1974, 1290]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [2294, 1464, 2402, 1547]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1887, 1188, 1924, 1304]\n",
                        "Class: car, Confidence: 0.40, Box: [859, 1783, 941, 1877]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 137.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_5.jpg: 640x640 3 persons, 5 cars, 2 motorcycles, 1 umbrella, 137.3ms\n",
                        "Speed: 3.0ms preprocess, 137.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 137.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1373 seconds\n",
                        "Class: car, Confidence: 0.96, Box: [109, 1114, 1603, 1903]\n",
                        "Class: car, Confidence: 0.91, Box: [3464, 1264, 3838, 1601]\n",
                        "Class: car, Confidence: 0.86, Box: [1609, 1227, 1818, 1412]\n",
                        "Class: car, Confidence: 0.84, Box: [1793, 1216, 1999, 1441]\n",
                        "Class: motorcycle, Confidence: 0.68, Box: [1975, 1254, 2159, 1565]\n",
                        "Class: person, Confidence: 0.66, Box: [1972, 1125, 2173, 1478]\n",
                        "Class: umbrella, Confidence: 0.66, Box: [2714, 1234, 2823, 1272]\n",
                        "Class: person, Confidence: 0.43, Box: [2744, 1268, 2800, 1381]\n",
                        "Class: car, Confidence: 0.27, Box: [1320, 1149, 1564, 1359]\n",
                        "Class: motorcycle, Confidence: 0.23, Box: [2130, 1238, 2211, 1452]\n",
                        "Class: person, Confidence: 0.22, Box: [2131, 1214, 2214, 1387]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 133.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_6.jpg: 640x640 3 cars, 133.9ms\n",
                        "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.0ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1339 seconds\n",
                        "Class: car, Confidence: 0.89, Box: [1284, 866, 1420, 982]\n",
                        "Class: car, Confidence: 0.82, Box: [1179, 856, 1250, 923]\n",
                        "Class: car, Confidence: 0.66, Box: [846, 870, 897, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 1 truck, 3 traffic lights, 164.9ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_7.jpg: 640x640 1 car, 1 truck, 3 traffic lights, 164.9ms\n",
                        "Speed: 3.5ms preprocess, 164.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.5ms preprocess, 164.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1649 seconds\n",
                        "Class: car, Confidence: 0.91, Box: [1304, 1640, 1769, 2007]\n",
                        "Class: traffic light, Confidence: 0.73, Box: [2447, 1259, 2472, 1318]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [2251, 1266, 2273, 1335]\n",
                        "Class: truck, Confidence: 0.53, Box: [2006, 1592, 2212, 1743]\n",
                        "Class: traffic light, Confidence: 0.33, Box: [1099, 1295, 1132, 1350]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 14 cars, 1 stop sign, 159.2ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_8.jpg: 640x640 14 cars, 1 stop sign, 159.2ms\n",
                        "Speed: 2.5ms preprocess, 159.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 159.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1592 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [2612, 1349, 3262, 1733]\n",
                        "Class: car, Confidence: 0.92, Box: [1784, 1359, 2130, 1611]\n",
                        "Class: car, Confidence: 0.85, Box: [1057, 1345, 1239, 1481]\n",
                        "Class: car, Confidence: 0.84, Box: [1688, 1325, 1956, 1542]\n",
                        "Class: car, Confidence: 0.75, Box: [1621, 1363, 1733, 1472]\n",
                        "Class: car, Confidence: 0.65, Box: [1567, 1348, 1669, 1455]\n",
                        "Class: stop sign, Confidence: 0.59, Box: [2553, 1144, 2611, 1269]\n",
                        "Class: car, Confidence: 0.58, Box: [1212, 1347, 1282, 1456]\n",
                        "Class: car, Confidence: 0.48, Box: [1346, 1350, 1403, 1399]\n",
                        "Class: car, Confidence: 0.48, Box: [1480, 1360, 1532, 1405]\n",
                        "Class: car, Confidence: 0.47, Box: [1245, 1344, 1319, 1433]\n",
                        "Class: car, Confidence: 0.35, Box: [1517, 1362, 1581, 1416]\n",
                        "Class: car, Confidence: 0.32, Box: [1530, 1360, 1592, 1425]\n",
                        "Class: car, Confidence: 0.26, Box: [1516, 1342, 1615, 1421]\n",
                        "Class: car, Confidence: 0.22, Box: [1295, 1351, 1335, 1420]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 3 cars, 121.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_9.jpg: 640x640 3 cars, 121.3ms\n",
                        "Speed: 2.5ms preprocess, 121.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 2.5ms preprocess, 121.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 0.1213 seconds\n",
                        "Class: car, Confidence: 0.52, Box: [1915, 1936, 1988, 1995]\n",
                        "Class: car, Confidence: 0.36, Box: [2418, 1906, 2508, 1944]\n",
                        "Class: car, Confidence: 0.25, Box: [2148, 1930, 2231, 1989]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1295 seconds\n",
                        "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
                        "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
                        "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
                        "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
                        "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
                        "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
                        "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
                        "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
                        "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
                        "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
                        "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1404 seconds\n",
                        "Class: traffic light, Confidence: 0.83, Box: [36, 448, 102, 579]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [513, 465, 564, 589]\n",
                        "Class: car, Confidence: 0.82, Box: [802, 1050, 899, 1115]\n",
                        "Class: car, Confidence: 0.81, Box: [970, 1034, 1066, 1117]\n",
                        "Class: traffic light, Confidence: 0.67, Box: [955, 515, 1045, 600]\n",
                        "Class: car, Confidence: 0.66, Box: [924, 1051, 958, 1081]\n",
                        "Class: truck, Confidence: 0.65, Box: [424, 1015, 708, 1217]\n",
                        "Class: car, Confidence: 0.63, Box: [381, 1058, 443, 1113]\n",
                        "Class: car, Confidence: 0.54, Box: [237, 1073, 321, 1115]\n",
                        "Class: car, Confidence: 0.47, Box: [734, 1052, 777, 1080]\n",
                        "Class: car, Confidence: 0.44, Box: [690, 1050, 732, 1083]\n",
                        "Class: traffic light, Confidence: 0.25, Box: [956, 518, 996, 598]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1829 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "Class: car, Confidence: 0.57, Box: [1981, 1750, 2289, 1864]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [1929, 1156, 1978, 1292]\n",
                        "Class: car, Confidence: 0.47, Box: [866, 1782, 952, 1877]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1886, 1190, 1928, 1303]\n",
                        "Class: car, Confidence: 0.29, Box: [3944, 1738, 3982, 1807]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1870 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [0, 2016, 659, 2356]\n",
                        "Class: car, Confidence: 0.90, Box: [815, 2100, 1316, 2297]\n",
                        "Class: person, Confidence: 0.84, Box: [1488, 2047, 1568, 2320]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [1346, 1261, 1408, 1429]\n",
                        "Class: traffic light, Confidence: 0.84, Box: [1907, 1281, 1967, 1447]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [1159, 1203, 1227, 1368]\n",
                        "Class: car, Confidence: 0.78, Box: [1795, 2214, 1917, 2288]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [3015, 773, 3161, 1041]\n",
                        "Class: traffic light, Confidence: 0.71, Box: [628, 1188, 699, 1355]\n",
                        "Class: traffic light, Confidence: 0.68, Box: [539, 1578, 602, 1692]\n",
                        "Class: traffic light, Confidence: 0.68, Box: [2751, 1027, 2872, 1242]\n",
                        "Class: car, Confidence: 0.66, Box: [3643, 2222, 4029, 2373]\n",
                        "Class: traffic light, Confidence: 0.63, Box: [411, 1513, 487, 1630]\n",
                        "Class: person, Confidence: 0.61, Box: [2209, 2169, 2266, 2290]\n",
                        "Class: person, Confidence: 0.60, Box: [2647, 2175, 2716, 2314]\n",
                        "Class: truck, Confidence: 0.53, Box: [2996, 2193, 3470, 2356]\n",
                        "Class: car, Confidence: 0.52, Box: [1297, 2159, 1410, 2256]\n",
                        "Class: traffic light, Confidence: 0.43, Box: [3472, 1418, 3716, 1608]\n",
                        "Class: car, Confidence: 0.37, Box: [1375, 2155, 1526, 2261]\n",
                        "Class: person, Confidence: 0.35, Box: [2155, 2158, 2205, 2289]\n",
                        "Class: person, Confidence: 0.32, Box: [2300, 2143, 2357, 2295]\n",
                        "Class: car, Confidence: 0.27, Box: [1369, 2154, 1658, 2264]\n",
                        "Class: car, Confidence: 0.27, Box: [1696, 2193, 1796, 2234]\n",
                        "Class: person, Confidence: 0.26, Box: [2401, 2210, 2475, 2314]\n",
                        "Class: car, Confidence: 0.23, Box: [1304, 2158, 1454, 2257]\n",
                        "Class: car, Confidence: 0.20, Box: [3644, 2242, 3768, 2365]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1866 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "Class: car, Confidence: 0.57, Box: [1981, 1750, 2289, 1864]\n",
                        "Class: traffic light, Confidence: 0.48, Box: [1929, 1156, 1978, 1292]\n",
                        "Class: car, Confidence: 0.47, Box: [866, 1782, 952, 1877]\n",
                        "Class: traffic light, Confidence: 0.45, Box: [1886, 1190, 1928, 1303]\n",
                        "Class: car, Confidence: 0.29, Box: [3944, 1738, 3982, 1807]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1675 seconds\n",
                        "Class: car, Confidence: 0.95, Box: [108, 1117, 1609, 1904]\n",
                        "Class: car, Confidence: 0.93, Box: [3464, 1269, 3837, 1596]\n",
                        "Class: car, Confidence: 0.84, Box: [1599, 1224, 1817, 1411]\n",
                        "Class: person, Confidence: 0.71, Box: [2727, 1256, 2806, 1381]\n",
                        "Class: umbrella, Confidence: 0.70, Box: [2713, 1235, 2824, 1267]\n",
                        "Class: person, Confidence: 0.68, Box: [1974, 1174, 2164, 1495]\n",
                        "Class: car, Confidence: 0.57, Box: [1790, 1214, 2005, 1443]\n",
                        "Class: motorcycle, Confidence: 0.53, Box: [1977, 1324, 2146, 1570]\n",
                        "Class: person, Confidence: 0.39, Box: [2129, 1205, 2219, 1453]\n",
                        "Class: car, Confidence: 0.31, Box: [1789, 1214, 2179, 1442]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1477 seconds\n",
                        "Class: car, Confidence: 0.84, Box: [1283, 865, 1423, 981]\n",
                        "Class: car, Confidence: 0.78, Box: [1179, 863, 1250, 923]\n",
                        "Class: car, Confidence: 0.74, Box: [846, 872, 895, 899]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1831 seconds\n",
                        "Class: car, Confidence: 0.92, Box: [1302, 1641, 1767, 2005]\n",
                        "Class: traffic light, Confidence: 0.71, Box: [2446, 1261, 2471, 1317]\n",
                        "Class: truck, Confidence: 0.58, Box: [2006, 1591, 2214, 1740]\n",
                        "Class: traffic light, Confidence: 0.56, Box: [2252, 1276, 2274, 1336]\n",
                        "Class: traffic light, Confidence: 0.25, Box: [1238, 1123, 1300, 1180]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1655 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [2608, 1350, 3261, 1731]\n",
                        "Class: car, Confidence: 0.92, Box: [1785, 1360, 2131, 1612]\n",
                        "Class: car, Confidence: 0.87, Box: [1686, 1327, 1956, 1542]\n",
                        "Class: car, Confidence: 0.84, Box: [1056, 1346, 1241, 1482]\n",
                        "Class: car, Confidence: 0.75, Box: [1617, 1365, 1735, 1471]\n",
                        "Class: car, Confidence: 0.66, Box: [1568, 1346, 1673, 1454]\n",
                        "Class: car, Confidence: 0.57, Box: [1513, 1344, 1611, 1421]\n",
                        "Class: car, Confidence: 0.54, Box: [1209, 1349, 1282, 1452]\n",
                        "Class: car, Confidence: 0.48, Box: [1238, 1346, 1326, 1435]\n",
                        "Class: stop sign, Confidence: 0.48, Box: [2553, 1145, 2610, 1267]\n",
                        "Class: car, Confidence: 0.46, Box: [1345, 1348, 1409, 1397]\n",
                        "Class: car, Confidence: 0.31, Box: [1479, 1357, 1536, 1395]\n",
                        "Class: car, Confidence: 0.29, Box: [1322, 1347, 1369, 1401]\n",
                        "Class: car, Confidence: 0.23, Box: [1243, 1343, 1337, 1421]\n",
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1799 seconds\n",
                        "Class: car, Confidence: 0.60, Box: [1914, 1936, 1989, 1994]\n",
                        "Class: car, Confidence: 0.53, Box: [2415, 1908, 2509, 1942]\n",
                        "Class: car, Confidence: 0.47, Box: [2146, 1929, 2234, 1995]\n",
                        "Class: traffic light, Confidence: 0.24, Box: [2274, 1636, 2340, 1700]\n"
                    ]
                }
            ],
            "source": [
                "def run_inference_with_args(inference_func, args):\n",
                "    def wrapper():\n",
                "        return inference_func(args)\n",
                "    return wrapper\n",
                "\n",
                "def collect_execution_times(run_inference_func, args, iterations=10):\n",
                "    execution_times = []\n",
                "    for i in range(iterations):\n",
                "        args[\"source\"] = f\"./app/assets/sample_image_{i}.jpg\"\n",
                "        wrapper_func = run_inference_with_args(run_inference_func, args)\n",
                "        execution_time = wrapper_func()\n",
                "        execution_times.append(execution_time[1] * 100)\n",
                "    return execution_times\n",
                "\n",
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\",\n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"score_threshold\": 0.1, \n",
                "    \"conf_threshold\": 0.2, \n",
                "    \"iou_threshold\": 0.6, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_pytorch))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_openvino))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_onnx))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.onnxruntime_run_image, args_onnx))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>pytorch time cpu (ms)</th>\n",
                            "      <th>openvino+ultralytics time cpu (ms)</th>\n",
                            "      <th>onnx​+ultralytics time cpu (ms)</th>\n",
                            "      <th>onnx runtime time cpu (ms)</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>count</th>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "      <td>10.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>mean</th>\n",
                            "      <td>12.351315</td>\n",
                            "      <td>12.286983</td>\n",
                            "      <td>14.374077</td>\n",
                            "      <td>16.701485</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>std</th>\n",
                            "      <td>0.967538</td>\n",
                            "      <td>0.239441</td>\n",
                            "      <td>1.927969</td>\n",
                            "      <td>2.097054</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>min</th>\n",
                            "      <td>11.183453</td>\n",
                            "      <td>12.054777</td>\n",
                            "      <td>12.133741</td>\n",
                            "      <td>12.947540</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>50%</th>\n",
                            "      <td>12.110066</td>\n",
                            "      <td>12.229943</td>\n",
                            "      <td>13.717365</td>\n",
                            "      <td>17.368840</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>90%</th>\n",
                            "      <td>13.303833</td>\n",
                            "      <td>12.481265</td>\n",
                            "      <td>16.658485</td>\n",
                            "      <td>18.666055</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>95%</th>\n",
                            "      <td>13.997581</td>\n",
                            "      <td>12.669148</td>\n",
                            "      <td>17.437989</td>\n",
                            "      <td>18.684933</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>max</th>\n",
                            "      <td>14.691329</td>\n",
                            "      <td>12.857032</td>\n",
                            "      <td>18.217492</td>\n",
                            "      <td>18.703810</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       pytorch time cpu (ms)  openvino+ultralytics time cpu (ms)  \\\n",
                            "count              10.000000                           10.000000   \n",
                            "mean               12.351315                           12.286983   \n",
                            "std                 0.967538                            0.239441   \n",
                            "min                11.183453                           12.054777   \n",
                            "50%                12.110066                           12.229943   \n",
                            "90%                13.303833                           12.481265   \n",
                            "95%                13.997581                           12.669148   \n",
                            "max                14.691329                           12.857032   \n",
                            "\n",
                            "       onnx​+ultralytics time cpu (ms)  onnx runtime time cpu (ms)  \n",
                            "count                        10.000000                   10.000000  \n",
                            "mean                         14.374077                   16.701485  \n",
                            "std                           1.927969                    2.097054  \n",
                            "min                          12.133741                   12.947540  \n",
                            "50%                          13.717365                   17.368840  \n",
                            "90%                          16.658485                   18.666055  \n",
                            "95%                          17.437989                   18.684933  \n",
                            "max                          18.217492                   18.703810  "
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "result_time = np.array(result_time)\n",
                "df = pd.DataFrame(np.transpose(result_time), \n",
                "                  columns=[\"pytorch time cpu (ms)\",\n",
                "                           \"openvino+ultralytics time cpu (ms)\",\n",
                "                           \"onnx​+ultralytics time cpu (ms)\", \n",
                "                           \"onnx runtime time cpu (ms)\"])\n",
                "df.describe(percentiles=[.9, .95])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save CPU result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('./app/saved_pkl/cpu_df.pkl', 'wb') as f:\n",
                "    pickle.dump(df, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "yolov9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

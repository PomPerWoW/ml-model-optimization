{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import pickle\n",
                "from app.util.timer import Timer\n",
                "from app.util.Differ import Differ\n",
                "from main import YoloRuntimeTest"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "CPU input"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx_simplified = {\n",
                "    \"weights\": \"./app/weights/yolov9c-simplified.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_2.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False,\n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Initilize YOLO runtime test class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "yolo_runtime_test = YoloRuntimeTest()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 149.6ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 480x640 5 cars, 6 traffic lights, 149.6ms\n",
                        "Speed: 2.0ms preprocess, 149.6ms inference, 497.7ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Speed: 2.0ms preprocess, 149.6ms inference, 497.7ms postprocess per image at shape (1, 3, 480, 640)\n",
                        "Elapsed time: 2.2091 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1689, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1493]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1226, 1475]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.75, Box: [1981, 1750, 2304, 1865]\n",
                        "Class: car, Confidence: 0.74, Box: [2340, 1729, 2527, 1813]\n",
                        "Class: car, Confidence: 0.74, Box: [3643, 1690, 3978, 1810]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9335812926292419, 859, 1689, 1509, 2235], ['car', 0.9225443601608276, 155, 1659, 620, 2021], ['traffic light', 0.9069226384162903, 366, 761, 480, 984], ['traffic light', 0.8241053819656372, 0, 769, 45, 995], ['traffic light', 0.7964197397232056, 1405, 1395, 1452, 1493], ['traffic light', 0.7787498831748962, 1181, 1379, 1226, 1475], ['traffic light', 0.7608105540275574, 991, 1367, 1041, 1467], ['car', 0.7513810396194458, 1981, 1750, 2304, 1865], ['car', 0.7438511848449707, 2340, 1729, 2527, 1813], ['car', 0.7359843254089355, 3643, 1690, 3978, 1810], ['traffic light', 0.7012598514556885, 1987, 1117, 2038, 1261]], 2.209140099999999)\n"
                    ]
                }
            ],
            "source": [
                "cpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
                "print(cpu_pytorch_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "Using OpenVINO LATENCY mode for batch=1 inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 131.4ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 131.4ms\n",
                        "Speed: 9.3ms preprocess, 131.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 9.3ms preprocess, 131.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.9008 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9310466051101685, 859, 1690, 1509, 2235], ['car', 0.921970009803772, 155, 1659, 620, 2021], ['traffic light', 0.9051550030708313, 366, 761, 480, 984], ['traffic light', 0.8343842625617981, 0, 769, 45, 995], ['traffic light', 0.7965543270111084, 1405, 1395, 1452, 1494], ['traffic light', 0.7801229953765869, 1181, 1379, 1227, 1475], ['traffic light', 0.7694882750511169, 991, 1367, 1041, 1467], ['car', 0.7377140522003174, 2339, 1730, 2529, 1814], ['car', 0.733312726020813, 3642, 1690, 3978, 1809], ['car', 0.7331985831260681, 1981, 1750, 2305, 1864], ['traffic light', 0.6988853216171265, 1987, 1117, 2038, 1261]], 1.9008446999999933)\n"
                    ]
                }
            ],
            "source": [
                "cpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
                "print(cpu_openvino_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 195.3ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 195.3ms\n",
                        "Speed: 3.5ms preprocess, 195.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.5ms preprocess, 195.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7779 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9310464262962341, 859, 1690, 1509, 2235], ['car', 0.9219695925712585, 155, 1659, 620, 2021], ['traffic light', 0.9051545858383179, 366, 761, 480, 984], ['traffic light', 0.8343836665153503, 0, 769, 45, 995], ['traffic light', 0.796554684638977, 1405, 1395, 1452, 1494], ['traffic light', 0.7801216840744019, 1181, 1379, 1227, 1475], ['traffic light', 0.7694879770278931, 991, 1367, 1041, 1467], ['car', 0.7377135753631592, 2339, 1730, 2529, 1814], ['car', 0.7333121299743652, 3642, 1690, 3978, 1809], ['car', 0.7331987619400024, 1981, 1750, 2305, 1864], ['traffic light', 0.6988840103149414, 1987, 1117, 2038, 1261]], 1.7779132999999945)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
                "print(cpu_onnx_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Loading app\\weights\\yolov9c-simplified.onnx for ONNX Runtime inference...\n",
                        "Loading app\\weights\\yolov9c-simplified.onnx for ONNX Runtime inference...\n",
                        "\n",
                        "\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 126.5ms\n",
                        "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_2.jpg: 640x640 5 cars, 6 traffic lights, 126.5ms\n",
                        "Speed: 3.0ms preprocess, 126.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Speed: 3.0ms preprocess, 126.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Elapsed time: 1.7032 seconds\n",
                        "Class: car, Confidence: 0.93, Box: [859, 1690, 1509, 2235]\n",
                        "Class: car, Confidence: 0.92, Box: [155, 1659, 620, 2021]\n",
                        "Class: traffic light, Confidence: 0.91, Box: [366, 761, 480, 984]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 769, 45, 995]\n",
                        "Class: traffic light, Confidence: 0.80, Box: [1405, 1395, 1452, 1494]\n",
                        "Class: traffic light, Confidence: 0.78, Box: [1181, 1379, 1227, 1475]\n",
                        "Class: traffic light, Confidence: 0.77, Box: [991, 1367, 1041, 1467]\n",
                        "Class: car, Confidence: 0.74, Box: [2339, 1730, 2529, 1814]\n",
                        "Class: car, Confidence: 0.73, Box: [3642, 1690, 3978, 1809]\n",
                        "Class: car, Confidence: 0.73, Box: [1981, 1750, 2305, 1864]\n",
                        "Class: traffic light, Confidence: 0.70, Box: [1987, 1117, 2038, 1261]\n",
                        "([['car', 0.9310464262962341, 859, 1690, 1509, 2235], ['car', 0.9219695925712585, 155, 1659, 620, 2021], ['traffic light', 0.9051545858383179, 366, 761, 480, 984], ['traffic light', 0.8343836665153503, 0, 769, 45, 995], ['traffic light', 0.796554684638977, 1405, 1395, 1452, 1494], ['traffic light', 0.7801216840744019, 1181, 1379, 1227, 1475], ['traffic light', 0.7694879770278931, 991, 1367, 1041, 1467], ['car', 0.7377135753631592, 2339, 1730, 2529, 1814], ['car', 0.7333121299743652, 3642, 1690, 3978, 1809], ['car', 0.7331987619400024, 1981, 1750, 2305, 1864], ['traffic light', 0.6988840103149414, 1987, 1117, 2038, 1261]], 1.7032460000000071)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_simplified_image = yolo_runtime_test.ultralytics_run_image(args_onnx_simplified)\n",
                "print(cpu_onnx_simplified_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1820 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "([['car', 0.9386053085327148, 856, 1686, 1511, 2235], ['traffic light', 0.9161757230758667, 366, 761, 480, 985], ['car', 0.9144351482391357, 154, 1658, 620, 2019], ['traffic light', 0.8307116031646729, 0, 768, 44, 997], ['traffic light', 0.8175124526023865, 991, 1367, 1042, 1466], ['traffic light', 0.8093212842941284, 1403, 1394, 1452, 1491], ['traffic light', 0.7603057622909546, 1180, 1380, 1226, 1475], ['car', 0.7564687728881836, 2338, 1730, 2529, 1814], ['traffic light', 0.6475574970245361, 1985, 1115, 2039, 1261], ['car', 0.6093098521232605, 3640, 1692, 3969, 1803]], 0.1820041999999944)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
                "print(cpu_onnx_runtime_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initialize Model\n",
                        "[INFO] Inference Image\n",
                        "Elapsed time: 0.1786 seconds\n",
                        "Class: car, Confidence: 0.94, Box: [856, 1686, 1511, 2235]\n",
                        "Class: traffic light, Confidence: 0.92, Box: [366, 761, 480, 985]\n",
                        "Class: car, Confidence: 0.91, Box: [154, 1658, 620, 2019]\n",
                        "Class: traffic light, Confidence: 0.83, Box: [0, 768, 44, 997]\n",
                        "Class: traffic light, Confidence: 0.82, Box: [991, 1367, 1042, 1466]\n",
                        "Class: traffic light, Confidence: 0.81, Box: [1403, 1394, 1452, 1491]\n",
                        "Class: traffic light, Confidence: 0.76, Box: [1180, 1380, 1226, 1475]\n",
                        "Class: car, Confidence: 0.76, Box: [2338, 1730, 2529, 1814]\n",
                        "Class: traffic light, Confidence: 0.65, Box: [1985, 1115, 2039, 1261]\n",
                        "Class: car, Confidence: 0.61, Box: [3640, 1692, 3969, 1803]\n",
                        "([['car', 0.9386053085327148, 856, 1686, 1511, 2235], ['traffic light', 0.9161757230758667, 366, 761, 480, 985], ['car', 0.9144351482391357, 154, 1658, 620, 2019], ['traffic light', 0.8307116031646729, 0, 768, 44, 997], ['traffic light', 0.8175124526023865, 991, 1367, 1042, 1466], ['traffic light', 0.8093212842941284, 1403, 1394, 1452, 1491], ['traffic light', 0.7603057622909546, 1180, 1380, 1226, 1475], ['car', 0.7564687728881836, 2338, 1730, 2529, 1814], ['traffic light', 0.6475574970245361, 1985, 1115, 2039, 1261], ['car', 0.6093098521232605, 3640, 1692, 3969, 1803]], 0.17864040000000614)\n"
                    ]
                }
            ],
            "source": [
                "cpu_onnx_simplified_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx_simplified)\n",
                "print(cpu_onnx_simplified_runtime_image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Difference CPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   pt vs openvino+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnx+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnxruntime cpu conf_diff           cpu box_diff (px)\n",
                        "0                         car        0.0025  [0.0, 1.0, 0.0, 0.0]                    car        0.0025  [0.0, 1.0, 0.0, 0.0]               car        0.0050        [3.0, 3.0, 2.0, 0.0]\n",
                        "1                         car        0.0006  [0.0, 0.0, 0.0, 0.0]                    car        0.0006  [0.0, 0.0, 0.0, 0.0]               car        0.0081        [1.0, 1.0, 0.0, 2.0]\n",
                        "2               traffic light        0.0018  [0.0, 0.0, 0.0, 0.0]          traffic light        0.0018  [0.0, 0.0, 0.0, 0.0]     traffic light        0.0093        [0.0, 0.0, 0.0, 1.0]\n",
                        "3               traffic light        0.0103  [0.0, 0.0, 0.0, 0.0]          traffic light        0.0103  [0.0, 0.0, 0.0, 0.0]     traffic light        0.0066        [0.0, 1.0, 1.0, 2.0]\n",
                        "4               traffic light        0.0001  [0.0, 0.0, 0.0, 1.0]          traffic light        0.0001  [0.0, 0.0, 0.0, 1.0]     traffic light        0.0129        [2.0, 1.0, 0.0, 2.0]\n",
                        "5               traffic light        0.0014  [0.0, 0.0, 1.0, 0.0]          traffic light        0.0014  [0.0, 0.0, 1.0, 0.0]     traffic light        0.0184        [1.0, 1.0, 0.0, 0.0]\n",
                        "6               traffic light        0.0087  [0.0, 0.0, 0.0, 0.0]          traffic light        0.0087  [0.0, 0.0, 0.0, 0.0]     traffic light        0.0567        [0.0, 0.0, 1.0, 1.0]\n",
                        "7                         car        0.0182  [0.0, 0.0, 1.0, 1.0]                    car        0.0182  [0.0, 0.0, 1.0, 1.0]               car        0.0051  [357.0, 20.0, 225.0, 51.0]\n",
                        "8                         car        0.0061  [1.0, 1.0, 2.0, 1.0]                    car        0.0061  [1.0, 1.0, 2.0, 1.0]               car        0.0126        [2.0, 1.0, 2.0, 1.0]\n",
                        "9                         car        0.0027  [1.0, 0.0, 0.0, 1.0]                    car        0.0027  [1.0, 0.0, 0.0, 1.0]               car        0.1267        [3.0, 2.0, 9.0, 7.0]\n",
                        "10              traffic light        0.0024  [0.0, 0.0, 0.0, 0.0]          traffic light        0.0024  [0.0, 0.0, 0.0, 0.0]     traffic light        0.0537        [2.0, 2.0, 1.0, 0.0]\n"
                    ]
                }
            ],
            "source": [
                "pd.set_option('display.expand_frame_repr', False)\n",
                "\n",
                "def generate_difference_df(image1, image2, label):\n",
                "    differ = Differ(np.array(image1), np.array(image2))\n",
                "    result = differ.find_difference()\n",
                "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff (px)\"])\n",
                "\n",
                "df_pt_openvino = generate_difference_df(cpu_pytorch_image[0], cpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
                "df_pt_onnx = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
                "df_pt_onnxruntime = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
                "\n",
                "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
                "\n",
                "print(df_combined)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Average CPU Time (10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result_time = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_inference_with_args(inference_func, args):\n",
                "    def wrapper():\n",
                "        return inference_func(args)\n",
                "    return wrapper\n",
                "\n",
                "def collect_execution_times(run_inference_func, args, iterations=10):\n",
                "    execution_times = []\n",
                "    for i in range(iterations):\n",
                "        args[\"source\"] = f\"./app/assets/sample_image_{i}.jpg\"\n",
                "        wrapper_func = run_inference_with_args(run_inference_func, args)\n",
                "        execution_time = wrapper_func()\n",
                "        execution_times.append(execution_time[1] * 100)\n",
                "    return execution_times\n",
                "\n",
                "args_pytorch = {\n",
                "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\",\n",
                "    \"classes\": \"./app/weights/metadata.yaml\", \n",
                "    \"type\": \"image\",\n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_onnx = {\n",
                "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "args_openvino = {\n",
                "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
                "    \"source\": \"./app/assets/sample_image_0.jpg\", \n",
                "    \"classes\": \"./app/weights/metadata.yaml\",\n",
                "    \"type\": \"image\", \n",
                "    \"show\": False, \n",
                "    \"conf_threshold\": 0.6, \n",
                "    \"iou_threshold\": 0.45, \n",
                "    \"device\": \"cpu\"\n",
                "}\n",
                "\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_pytorch))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_openvino))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.ultralytics_run_image, args_onnx))\n",
                "result_time.append(collect_execution_times(yolo_runtime_test.onnxruntime_run_image, args_onnx))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result_time = np.array(result_time)\n",
                "df = pd.DataFrame(np.transpose(result_time), \n",
                "                  columns=[\"pytorch time cpu (ms)\",\n",
                "                           \"openvino+ultralytics time cpu (ms)\",\n",
                "                           \"onnx​+ultralytics time cpu (ms)\", \n",
                "                           \"onnx runtime time cpu (ms)\"])\n",
                "df.describe(percentiles=[.9, .95])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save CPU result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('./app/saved_pkl/cpu_df.pkl', 'wb') as f:\n",
                "    pickle.dump(df, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "yolov9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from app.util.timer import Timer\n",
    "from app.util.util import Differ\n",
    "from main import YoloRuntimeTest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch = {\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\",\n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "args_onnx = {\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "args_openvino = {\n",
    "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize YOLO runtime test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_runtime_test = YoloRuntimeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6410 seconds\n",
      "([['car', 0.9299132823944092, 558, 206, 808, 359], ['car', 0.9205407500267029, 286, 210, 458, 352], ['car', 0.9116201996803284, 465, 217, 596, 339], ['person', 0.8726341724395752, 159, 143, 301, 403], ['truck', 0.8677411079406738, 103, 90, 255, 316], ['truck', 0.7794504761695862, 722, 170, 871, 346], ['truck', 0.7472285628318787, 0, 154, 94, 354], ['bicycle', 0.651627779006958, 210, 321, 266, 443], ['car', 0.5186470150947571, 78, 212, 113, 300], ['car', 0.36083880066871643, 420, 226, 474, 319], ['car', 0.3002343475818634, 420, 227, 464, 278]], 1.6410321000003023)\n"
     ]
    }
   ],
   "source": [
    "cpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
    "print(cpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 141.4ms\n",
      "Speed: 1.5ms preprocess, 141.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7598 seconds\n",
      "([['car', 0.9305108189582825, 558, 206, 808, 359], ['car', 0.9209165573120117, 286, 210, 458, 352], ['car', 0.9119921326637268, 465, 217, 596, 339], ['person', 0.8731657266616821, 159, 143, 301, 403], ['truck', 0.868000864982605, 103, 89, 255, 316], ['truck', 0.7941579818725586, 722, 170, 871, 346], ['truck', 0.74634850025177, 0, 154, 94, 354], ['bicycle', 0.6512935757637024, 210, 321, 266, 443], ['car', 0.5251946449279785, 78, 212, 113, 300], ['car', 0.3632618486881256, 420, 226, 474, 319], ['car', 0.2982390820980072, 420, 227, 464, 278], ['traffic light', 0.21935202181339264, 258, 82, 274, 116]], 1.7598307999996905)\n"
     ]
    }
   ],
   "source": [
    "cpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
    "print(cpu_openvino_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 192.6ms\n",
      "Speed: 1.0ms preprocess, 192.6ms inference, 24.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7578 seconds\n",
      "([['car', 0.9305105805397034, 558, 206, 808, 359], ['car', 0.9209166169166565, 286, 210, 458, 352], ['car', 0.9119920134544373, 465, 217, 596, 339], ['person', 0.8731658458709717, 159, 143, 301, 403], ['truck', 0.8680006265640259, 103, 89, 255, 316], ['truck', 0.7941577434539795, 722, 170, 871, 346], ['truck', 0.7463480830192566, 0, 154, 94, 354], ['bicycle', 0.6512946486473083, 210, 321, 266, 443], ['car', 0.5251938104629517, 78, 212, 113, 300], ['car', 0.36326032876968384, 420, 226, 474, 319], ['car', 0.2982402443885803, 420, 227, 464, 278], ['traffic light', 0.21935239434242249, 258, 82, 274, 116]], 1.7578248999998323)\n"
     ]
    }
   ],
   "source": [
    "cpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
    "print(cpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.2016 seconds\n",
      "([['car', 0.9232816696166992, 556, 206, 810, 359], ['car', 0.916867733001709, 463, 217, 595, 338], ['car', 0.9103224277496338, 286, 210, 459, 351], ['person', 0.9022834300994873, 159, 143, 299, 403], ['truck', 0.8684066534042358, 723, 171, 871, 345], ['truck', 0.8390960693359375, 102, 89, 257, 314], ['bicycle', 0.724895179271698, 209, 322, 269, 441], ['truck', 0.6845371723175049, 0, 154, 93, 354], ['car', 0.39963772892951965, 78, 225, 113, 300], ['car', 0.22634947299957275, 421, 225, 483, 268], ['car', 0.21756017208099365, 421, 229, 470, 320]], 0.20158160000028147)\n"
     ]
    }
   ],
   "source": [
    "cpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
    "print(cpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pt vs openvino+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnx+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnxruntime cpu conf_diff       cpu box_diff (px)\n",
      "0                         car        0.0006  [0.0, 0.0, 0.0, 0.0]                    car        0.0006  [0.0, 0.0, 0.0, 0.0]               car        0.0066    [2.0, 0.0, 2.0, 0.0]\n",
      "1                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0102    [0.0, 0.0, 1.0, 1.0]\n",
      "2                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0052    [2.0, 0.0, 1.0, 1.0]\n",
      "3                      person        0.0005  [0.0, 0.0, 0.0, 0.0]                 person        0.0005  [0.0, 0.0, 0.0, 0.0]            person        0.0296    [0.0, 0.0, 2.0, 0.0]\n",
      "4                       truck        0.0003  [0.0, 1.0, 0.0, 0.0]                  truck        0.0003  [0.0, 1.0, 0.0, 0.0]             truck        0.0286    [1.0, 1.0, 2.0, 2.0]\n",
      "5                       truck        0.0147  [0.0, 0.0, 0.0, 0.0]                  truck        0.0147  [0.0, 0.0, 0.0, 0.0]             truck        0.0890    [1.0, 1.0, 0.0, 1.0]\n",
      "6                       truck        0.0009  [0.0, 0.0, 0.0, 0.0]                  truck        0.0009  [0.0, 0.0, 0.0, 0.0]             truck        0.0627    [0.0, 0.0, 1.0, 0.0]\n",
      "7                     bicycle        0.0003  [0.0, 0.0, 0.0, 0.0]                bicycle        0.0003  [0.0, 0.0, 0.0, 0.0]           bicycle        0.0733    [1.0, 1.0, 3.0, 2.0]\n",
      "8                         car        0.0065  [0.0, 0.0, 0.0, 0.0]                    car        0.0065  [0.0, 0.0, 0.0, 0.0]               car        0.1190   [0.0, 13.0, 0.0, 0.0]\n",
      "9                         car        0.0024  [0.0, 0.0, 0.0, 0.0]                    car        0.0024  [0.0, 0.0, 0.0, 0.0]               car        0.1433    [1.0, 3.0, 4.0, 1.0]\n",
      "10                        car        0.0020  [0.0, 0.0, 0.0, 0.0]                    car        0.0020  [0.0, 0.0, 0.0, 0.0]               car        0.0739  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff (px)\"])\n",
    "\n",
    "df_pt_openvino = generate_difference_df(cpu_pytorch_image[0], cpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
    "df_pt_onnx = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
    "df_pt_onnxruntime = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CPU Time (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 148.7ms\n",
      "Speed: 2.0ms preprocess, 148.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6286 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 137.0ms\n",
      "Speed: 2.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6328 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 131.1ms\n",
      "Speed: 1.0ms preprocess, 131.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6419 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 127.1ms\n",
      "Speed: 1.0ms preprocess, 127.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6048 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 144.6ms\n",
      "Speed: 1.0ms preprocess, 144.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6495 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 145.8ms\n",
      "Speed: 1.0ms preprocess, 145.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6378 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 154.7ms\n",
      "Speed: 2.0ms preprocess, 154.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6701 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 128.4ms\n",
      "Speed: 1.0ms preprocess, 128.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.7148 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 124.4ms\n",
      "Speed: 1.0ms preprocess, 124.4ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6055 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 129.4ms\n",
      "Speed: 1.0ms preprocess, 129.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6108 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 133.8ms\n",
      "Speed: 4.3ms preprocess, 133.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8331 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 133.1ms\n",
      "Speed: 2.0ms preprocess, 133.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7298 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 131.5ms\n",
      "Speed: 3.0ms preprocess, 131.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7859 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.4ms\n",
      "Speed: 1.0ms preprocess, 128.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8350 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 156.1ms\n",
      "Speed: 1.0ms preprocess, 156.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7285 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 131.1ms\n",
      "Speed: 1.0ms preprocess, 131.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7078 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 137.4ms\n",
      "Speed: 2.0ms preprocess, 137.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8111 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 131.5ms\n",
      "Speed: 2.0ms preprocess, 131.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7305 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 129.6ms\n",
      "Speed: 3.0ms preprocess, 129.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7259 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 136.4ms\n",
      "Speed: 1.0ms preprocess, 136.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7143 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 174.4ms\n",
      "Speed: 2.0ms preprocess, 174.4ms inference, 33.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7780 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 237.6ms\n",
      "Speed: 2.0ms preprocess, 237.6ms inference, 31.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8358 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 186.9ms\n",
      "Speed: 2.0ms preprocess, 186.9ms inference, 22.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7647 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 186.8ms\n",
      "Speed: 2.5ms preprocess, 186.8ms inference, 25.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7772 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 163.6ms\n",
      "Speed: 2.5ms preprocess, 163.6ms inference, 21.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7299 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 221.3ms\n",
      "Speed: 2.0ms preprocess, 221.3ms inference, 26.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8013 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 210.4ms\n",
      "Speed: 2.0ms preprocess, 210.4ms inference, 24.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8133 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 219.1ms\n",
      "Speed: 2.0ms preprocess, 219.1ms inference, 21.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8109 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 233.4ms\n",
      "Speed: 3.0ms preprocess, 233.4ms inference, 21.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8200 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 144.6ms\n",
      "Speed: 1.6ms preprocess, 144.6ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8727 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1419 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1657 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1926 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1513 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1952 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1657 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1582 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1428 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1776 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1415 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "pytorch_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_pytorch)\n",
    "openvino_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_openvino)\n",
    "onnx_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_onnx)\n",
    "onnx_runtime_func = partial(yolo_runtime_test.onnxruntime_run_image, args=args_onnx)\n",
    "\n",
    "result_time.append(collect_execution_times(pytorch_func))\n",
    "result_time.append(collect_execution_times(openvino_func))\n",
    "result_time.append(collect_execution_times(onnx_func))\n",
    "result_time.append(collect_execution_times(onnx_runtime_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time cpu (ms)</th>\n",
       "      <th>openvino+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx runtime time cpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>163.965937</td>\n",
       "      <td>176.019052</td>\n",
       "      <td>180.036874</td>\n",
       "      <td>16.326629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.343268</td>\n",
       "      <td>5.055348</td>\n",
       "      <td>4.004296</td>\n",
       "      <td>2.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>160.476280</td>\n",
       "      <td>170.782670</td>\n",
       "      <td>172.986000</td>\n",
       "      <td>14.154320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>163.531245</td>\n",
       "      <td>173.013945</td>\n",
       "      <td>180.607330</td>\n",
       "      <td>16.194910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>167.457046</td>\n",
       "      <td>183.332319</td>\n",
       "      <td>183.946278</td>\n",
       "      <td>19.289799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>169.467268</td>\n",
       "      <td>183.414934</td>\n",
       "      <td>185.609829</td>\n",
       "      <td>19.404095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>171.477490</td>\n",
       "      <td>183.497550</td>\n",
       "      <td>187.273380</td>\n",
       "      <td>19.518390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time cpu (ms)  openvino+ultralytics time cpu (ms)  onnx+ultralytics time cpu (ms)  onnx runtime time cpu (ms)\n",
       "count              10.000000                           10.000000                        10.000000                   10.000000\n",
       "mean              163.965937                          176.019052                       180.036874                   16.326629\n",
       "std                 3.343268                            5.055348                         4.004296                    2.003532\n",
       "min               160.476280                          170.782670                       172.986000                   14.154320\n",
       "50%               163.531245                          173.013945                       180.607330                   16.194910\n",
       "90%               167.457046                          183.332319                       183.946278                   19.289799\n",
       "95%               169.467268                          183.414934                       185.609829                   19.404095\n",
       "max               171.477490                          183.497550                       187.273380                   19.518390"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time cpu (ms)\",\n",
    "                           \"openvino+ultralytics time cpu (ms)\",\n",
    "                           \"onnx+ultralytics time cpu (ms)\", \n",
    "                           \"onnx runtime time cpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save CPU result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./app/saved_pkl/cpu_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

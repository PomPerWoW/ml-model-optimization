{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from app.util.timer import Timer\n",
    "from app.util.util import Differ\n",
    "from main import YoloRuntimeTest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch = {\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\",\n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "args_onnx = {\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "args_openvino = {\n",
    "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize YOLO runtime test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_runtime_test = YoloRuntimeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 133.0ms\n",
      "Speed: 1.0ms preprocess, 133.0ms inference, 505.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 2.1441 seconds\n",
      "([['car', 0.9299132823944092, 558, 206, 808, 359], ['car', 0.9205407500267029, 286, 210, 458, 352], ['car', 0.9116201996803284, 465, 217, 596, 339], ['person', 0.8726341724395752, 159, 143, 301, 403], ['truck', 0.8677411079406738, 103, 90, 255, 316], ['truck', 0.7794504761695862, 722, 170, 871, 346], ['truck', 0.7472285628318787, 0, 154, 94, 354], ['bicycle', 0.651627779006958, 210, 321, 266, 443], ['car', 0.5186470150947571, 78, 212, 113, 300], ['car', 0.36083880066871643, 420, 226, 474, 319], ['car', 0.3002343475818634, 420, 227, 464, 278]], 2.1441063)\n"
     ]
    }
   ],
   "source": [
    "cpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
    "print(cpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 119.1ms\n",
      "Speed: 2.0ms preprocess, 119.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7195 seconds\n",
      "([['car', 0.9305108189582825, 558, 206, 808, 359], ['car', 0.9209165573120117, 286, 210, 458, 352], ['car', 0.9119921326637268, 465, 217, 596, 339], ['person', 0.8731657266616821, 159, 143, 301, 403], ['truck', 0.868000864982605, 103, 89, 255, 316], ['truck', 0.7941579818725586, 722, 170, 871, 346], ['truck', 0.74634850025177, 0, 154, 94, 354], ['bicycle', 0.6512935757637024, 210, 321, 266, 443], ['car', 0.5251946449279785, 78, 212, 113, 300], ['car', 0.3632618486881256, 420, 226, 474, 319], ['car', 0.2982390820980072, 420, 227, 464, 278], ['traffic light', 0.21935202181339264, 258, 82, 274, 116]], 1.7194683)\n"
     ]
    }
   ],
   "source": [
    "cpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
    "print(cpu_openvino_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.6637 seconds\n",
      "([['car', 0.9305105805397034, 558, 206, 808, 359], ['car', 0.9209166169166565, 286, 210, 458, 352], ['car', 0.9119920134544373, 465, 217, 596, 339], ['person', 0.8731658458709717, 159, 143, 301, 403], ['truck', 0.8680006265640259, 103, 89, 255, 316], ['truck', 0.7941577434539795, 722, 170, 871, 346], ['truck', 0.7463480830192566, 0, 154, 94, 354], ['bicycle', 0.6512946486473083, 210, 321, 266, 443], ['car', 0.5251938104629517, 78, 212, 113, 300], ['car', 0.36326032876968384, 420, 226, 474, 319], ['car', 0.2982402443885803, 420, 227, 464, 278], ['traffic light', 0.21935239434242249, 258, 82, 274, 116]], 1.6636849999999992)\n"
     ]
    }
   ],
   "source": [
    "cpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
    "print(cpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1293 seconds\n",
      "([['car', 0.9232816696166992, 556, 206, 810, 359], ['car', 0.916867733001709, 463, 217, 595, 338], ['car', 0.9103224277496338, 286, 210, 459, 351], ['person', 0.9022834300994873, 159, 143, 299, 403], ['truck', 0.8684066534042358, 723, 171, 871, 345], ['truck', 0.8390960693359375, 102, 89, 257, 314], ['bicycle', 0.724895179271698, 209, 322, 269, 441], ['truck', 0.6845371723175049, 0, 154, 93, 354], ['car', 0.39963772892951965, 78, 225, 113, 300], ['car', 0.22634947299957275, 421, 225, 483, 268], ['car', 0.21756017208099365, 421, 229, 470, 320]], 0.12926440000000028)\n"
     ]
    }
   ],
   "source": [
    "cpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
    "print(cpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pt vs openvino+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnx+ultralytics cpu conf_diff     cpu box_diff (px) pt vs onnxruntime cpu conf_diff       cpu box_diff (px)\n",
      "0                         car        0.0006  [0.0, 0.0, 0.0, 0.0]                    car        0.0006  [0.0, 0.0, 0.0, 0.0]               car        0.0066    [2.0, 0.0, 2.0, 0.0]\n",
      "1                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0102    [0.0, 0.0, 1.0, 1.0]\n",
      "2                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0052    [2.0, 0.0, 1.0, 1.0]\n",
      "3                      person        0.0005  [0.0, 0.0, 0.0, 0.0]                 person        0.0005  [0.0, 0.0, 0.0, 0.0]            person        0.0296    [0.0, 0.0, 2.0, 0.0]\n",
      "4                       truck        0.0003  [0.0, 1.0, 0.0, 0.0]                  truck        0.0003  [0.0, 1.0, 0.0, 0.0]             truck        0.0286    [1.0, 1.0, 2.0, 2.0]\n",
      "5                       truck        0.0147  [0.0, 0.0, 0.0, 0.0]                  truck        0.0147  [0.0, 0.0, 0.0, 0.0]             truck        0.0890    [1.0, 1.0, 0.0, 1.0]\n",
      "6                       truck        0.0009  [0.0, 0.0, 0.0, 0.0]                  truck        0.0009  [0.0, 0.0, 0.0, 0.0]             truck        0.0627    [0.0, 0.0, 1.0, 0.0]\n",
      "7                     bicycle        0.0003  [0.0, 0.0, 0.0, 0.0]                bicycle        0.0003  [0.0, 0.0, 0.0, 0.0]           bicycle        0.0733    [1.0, 1.0, 3.0, 2.0]\n",
      "8                         car        0.0065  [0.0, 0.0, 0.0, 0.0]                    car        0.0065  [0.0, 0.0, 0.0, 0.0]               car        0.1190   [0.0, 13.0, 0.0, 0.0]\n",
      "9                         car        0.0024  [0.0, 0.0, 0.0, 0.0]                    car        0.0024  [0.0, 0.0, 0.0, 0.0]               car        0.1433    [1.0, 3.0, 4.0, 1.0]\n",
      "10                        car        0.0020  [0.0, 0.0, 0.0, 0.0]                    car        0.0020  [0.0, 0.0, 0.0, 0.0]               car        0.0739  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff (px)\"])\n",
    "\n",
    "df_pt_openvino = generate_difference_df(cpu_pytorch_image[0], cpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
    "df_pt_onnx = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
    "df_pt_onnxruntime = generate_difference_df(cpu_pytorch_image[0], cpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CPU Time (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 125.6ms\n",
      "Speed: 1.5ms preprocess, 125.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6509 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 127.6ms\n",
      "Speed: 1.0ms preprocess, 127.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6390 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 125.9ms\n",
      "Speed: 1.0ms preprocess, 125.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6400 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 128.8ms\n",
      "Speed: 2.0ms preprocess, 128.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6524 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 127.2ms\n",
      "Speed: 1.0ms preprocess, 127.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6394 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 127.3ms\n",
      "Speed: 1.0ms preprocess, 127.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6284 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 119.1ms\n",
      "Speed: 1.0ms preprocess, 119.1ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6279 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 119.3ms\n",
      "Speed: 1.0ms preprocess, 119.3ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6280 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 116.9ms\n",
      "Speed: 1.0ms preprocess, 116.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6171 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 116.8ms\n",
      "Speed: 2.0ms preprocess, 116.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6228 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 138.9ms\n",
      "Speed: 2.0ms preprocess, 138.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8010 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 130.4ms\n",
      "Speed: 2.5ms preprocess, 130.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7435 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 129.2ms\n",
      "Speed: 1.0ms preprocess, 129.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7328 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 130.1ms\n",
      "Speed: 2.0ms preprocess, 130.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7394 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.8299 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 129.4ms\n",
      "Speed: 2.0ms preprocess, 129.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7275 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 133.4ms\n",
      "Speed: 2.0ms preprocess, 133.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7245 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 135.3ms\n",
      "Speed: 1.0ms preprocess, 135.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7491 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 126.2ms\n",
      "Speed: 1.0ms preprocess, 126.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7143 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.5ms\n",
      "Speed: 1.5ms preprocess, 128.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7585 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 145.1ms\n",
      "Speed: 2.0ms preprocess, 145.1ms inference, 22.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7223 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 157.6ms\n",
      "Speed: 2.5ms preprocess, 157.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7215 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 130.4ms\n",
      "Speed: 2.5ms preprocess, 130.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7261 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 143.5ms\n",
      "Speed: 3.0ms preprocess, 143.5ms inference, 15.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7353 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 163.9ms\n",
      "Speed: 2.5ms preprocess, 163.9ms inference, 19.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7645 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 133.4ms\n",
      "Speed: 2.0ms preprocess, 133.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7170 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 180.1ms\n",
      "Speed: 2.0ms preprocess, 180.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7516 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.1ms\n",
      "Speed: 1.0ms preprocess, 128.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7149 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.0ms\n",
      "Speed: 2.0ms preprocess, 128.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.7134 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 126.6ms\n",
      "Speed: 2.5ms preprocess, 126.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 1.9057 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1264 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1270 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1290 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1280 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1287 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1283 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1481 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1290 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1288 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1282 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "pytorch_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_pytorch)\n",
    "openvino_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_openvino)\n",
    "onnx_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_onnx)\n",
    "onnx_runtime_func = partial(yolo_runtime_test.onnxruntime_run_image, args=args_onnx)\n",
    "\n",
    "result_time.append(collect_execution_times(pytorch_func))\n",
    "result_time.append(collect_execution_times(openvino_func))\n",
    "result_time.append(collect_execution_times(onnx_func))\n",
    "result_time.append(collect_execution_times(onnx_runtime_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time cpu (ms)</th>\n",
       "      <th>openvino+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx runtime time cpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>163.460068</td>\n",
       "      <td>175.205051</td>\n",
       "      <td>174.723227</td>\n",
       "      <td>13.015163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.165699</td>\n",
       "      <td>3.634362</td>\n",
       "      <td>5.813205</td>\n",
       "      <td>0.635195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>161.714140</td>\n",
       "      <td>171.432390</td>\n",
       "      <td>171.343010</td>\n",
       "      <td>12.643990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>163.370355</td>\n",
       "      <td>174.146355</td>\n",
       "      <td>172.419035</td>\n",
       "      <td>12.850185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>165.101633</td>\n",
       "      <td>180.387531</td>\n",
       "      <td>177.859403</td>\n",
       "      <td>13.093971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>165.172252</td>\n",
       "      <td>181.687541</td>\n",
       "      <td>184.216567</td>\n",
       "      <td>13.950551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>165.242870</td>\n",
       "      <td>182.987550</td>\n",
       "      <td>190.573730</td>\n",
       "      <td>14.807130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time cpu (ms)  openvino+ultralytics time cpu (ms)  onnx+ultralytics time cpu (ms)  onnx runtime time cpu (ms)\n",
       "count              10.000000                           10.000000                        10.000000                   10.000000\n",
       "mean              163.460068                          175.205051                       174.723227                   13.015163\n",
       "std                 1.165699                            3.634362                         5.813205                    0.635195\n",
       "min               161.714140                          171.432390                       171.343010                   12.643990\n",
       "50%               163.370355                          174.146355                       172.419035                   12.850185\n",
       "90%               165.101633                          180.387531                       177.859403                   13.093971\n",
       "95%               165.172252                          181.687541                       184.216567                   13.950551\n",
       "max               165.242870                          182.987550                       190.573730                   14.807130"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time cpu (ms)\",\n",
    "                           \"openvino+ultralytics time cpu (ms)\",\n",
    "                           \"onnx+ultralytics time cpu (ms)\", \n",
    "                           \"onnx runtime time cpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

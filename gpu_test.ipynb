{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from app.util.timer import Timer\n",
    "from app.util.Differ import Differ\n",
    "from main import YoloRuntimeTest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch = {\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image_3.jpg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\",\n",
    "    \"show\": False, \n",
    "    \"conf_threshold\": 0.25, \n",
    "    \"iou_threshold\": 0.45, \n",
    "    \"device\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "args_onnx = {\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image_3.jpg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"conf_threshold\": 0.25, \n",
    "    \"iou_threshold\": 0.45, \n",
    "    \"device\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "args_openvino = {\n",
    "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
    "    \"source\": \"./app/assets/sample_image_3.jpg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"conf_threshold\": 0.25, \n",
    "    \"iou_threshold\": 0.45, \n",
    "    \"device\": \"cuda:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize YOLO runtime test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_runtime_test = YoloRuntimeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "\n",
      "infer time: 0.1843855000000012 s\n",
      "infer time: 0.0957111000000026 s\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 6 persons, 8 cars, 1 truck, 9 traffic lights, 96.2ms\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 480x640 6 persons, 8 cars, 1 truck, 9 traffic lights, 96.2ms\n",
      "Speed: 3.5ms preprocess, 96.2ms inference, 529.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Speed: 3.5ms preprocess, 96.2ms inference, 529.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Elapsed time: 0.0962 seconds\n",
      "Class: car, Confidence: 0.93, Box: [1, 2018, 664, 2355]\n",
      "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
      "Class: traffic light, Confidence: 0.82, Box: [1346, 1263, 1408, 1428]\n",
      "Class: traffic light, Confidence: 0.82, Box: [1907, 1279, 1966, 1447]\n",
      "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2324]\n",
      "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
      "Class: traffic light, Confidence: 0.79, Box: [3014, 773, 3155, 1041]\n",
      "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
      "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
      "Class: traffic light, Confidence: 0.66, Box: [411, 1513, 488, 1640]\n",
      "Class: car, Confidence: 0.64, Box: [2998, 2193, 3468, 2359]\n",
      "Class: traffic light, Confidence: 0.57, Box: [2753, 1024, 2891, 1246]\n",
      "Class: car, Confidence: 0.51, Box: [3646, 2242, 3771, 2366]\n",
      "Class: truck, Confidence: 0.45, Box: [2994, 2192, 3465, 2360]\n",
      "Class: person, Confidence: 0.44, Box: [2219, 2174, 2260, 2291]\n",
      "Class: person, Confidence: 0.43, Box: [2002, 2151, 2063, 2300]\n",
      "Class: car, Confidence: 0.43, Box: [1382, 2157, 1516, 2258]\n",
      "Class: car, Confidence: 0.40, Box: [1300, 2156, 1402, 2255]\n",
      "Class: traffic light, Confidence: 0.39, Box: [3474, 1415, 3714, 1612]\n",
      "Class: person, Confidence: 0.39, Box: [2649, 2170, 2705, 2315]\n",
      "Class: person, Confidence: 0.37, Box: [2304, 2142, 2352, 2297]\n",
      "Class: car, Confidence: 0.29, Box: [1301, 2154, 1522, 2260]\n",
      "Class: traffic light, Confidence: 0.28, Box: [540, 1570, 606, 1698]\n",
      "Class: person, Confidence: 0.27, Box: [2160, 2167, 2206, 2291]\n",
      "([['car', 0.9271767735481262, 1, 2018, 664, 2355], ['car', 0.9052380323410034, 815, 2097, 1315, 2295], ['traffic light', 0.823891818523407, 1346, 1263, 1408, 1428], ['traffic light', 0.8213894963264465, 1907, 1279, 1966, 1447], ['person', 0.8199774026870728, 1487, 2043, 1569, 2324], ['traffic light', 0.8113959431648254, 1162, 1202, 1228, 1366], ['traffic light', 0.7884622812271118, 3014, 773, 3155, 1041], ['traffic light', 0.7594167590141296, 629, 1187, 698, 1353], ['car', 0.713955819606781, 1799, 2216, 1914, 2290], ['traffic light', 0.6624890565872192, 411, 1513, 488, 1640], ['car', 0.637826681137085, 2998, 2193, 3468, 2359], ['traffic light', 0.5698794722557068, 2753, 1024, 2891, 1246], ['car', 0.514707088470459, 3646, 2242, 3771, 2366], ['truck', 0.44730445742607117, 2994, 2192, 3465, 2360], ['person', 0.44232869148254395, 2219, 2174, 2260, 2291], ['person', 0.4276718199253082, 2002, 2151, 2063, 2300], ['car', 0.4270835518836975, 1382, 2157, 1516, 2258], ['car', 0.39931172132492065, 1300, 2156, 1402, 2255], ['traffic light', 0.3933943510055542, 3474, 1415, 3714, 1612], ['person', 0.3917250335216522, 2649, 2170, 2705, 2315], ['person', 0.3741733431816101, 2304, 2142, 2352, 2297], ['car', 0.2887345850467682, 1301, 2154, 1522, 2260], ['traffic light', 0.28444573283195496, 540, 1570, 606, 1698], ['person', 0.274902880191803, 2160, 2167, 2206, 2291]], 0.09621834754943848)\n"
     ]
    }
   ],
   "source": [
    "gpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
    "print(gpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 122.8ms\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 122.8ms\n",
      "Speed: 1.0ms preprocess, 122.8ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 1.0ms preprocess, 122.8ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Elapsed time: 0.1228 seconds\n",
      "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
      "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
      "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
      "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
      "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
      "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
      "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
      "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
      "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
      "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
      "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
      "Class: traffic light, Confidence: 0.55, Box: [2753, 1024, 2889, 1246]\n",
      "Class: car, Confidence: 0.52, Box: [3646, 2242, 3771, 2365]\n",
      "Class: car, Confidence: 0.46, Box: [1382, 2156, 1514, 2258]\n",
      "Class: person, Confidence: 0.45, Box: [2219, 2174, 2261, 2290]\n",
      "Class: car, Confidence: 0.44, Box: [1300, 2155, 1401, 2255]\n",
      "Class: truck, Confidence: 0.43, Box: [2993, 2192, 3465, 2360]\n",
      "Class: person, Confidence: 0.43, Box: [2003, 2150, 2063, 2300]\n",
      "Class: person, Confidence: 0.39, Box: [2303, 2141, 2352, 2297]\n",
      "Class: traffic light, Confidence: 0.38, Box: [3473, 1414, 3716, 1612]\n",
      "Class: person, Confidence: 0.38, Box: [2649, 2169, 2706, 2315]\n",
      "Class: car, Confidence: 0.28, Box: [1301, 2154, 1521, 2259]\n",
      "Class: traffic light, Confidence: 0.28, Box: [540, 1571, 606, 1699]\n",
      "Class: person, Confidence: 0.27, Box: [2160, 2168, 2206, 2291]\n",
      "Class: traffic light, Confidence: 0.23, Box: [2963, 1034, 3178, 1224]\n",
      "([['car', 0.9288111329078674, 1, 2018, 663, 2354], ['car', 0.9077546000480652, 815, 2097, 1315, 2295], ['person', 0.8231940269470215, 1487, 2043, 1569, 2323], ['traffic light', 0.8192124962806702, 1346, 1262, 1409, 1428], ['traffic light', 0.8145473599433899, 1907, 1279, 1967, 1447], ['traffic light', 0.8084031939506531, 1162, 1202, 1228, 1366], ['traffic light', 0.8003724813461304, 3014, 773, 3155, 1041], ['traffic light', 0.7566177248954773, 629, 1187, 698, 1353], ['car', 0.7073200345039368, 1799, 2216, 1914, 2290], ['traffic light', 0.6712169051170349, 411, 1513, 488, 1638], ['car', 0.6341890692710876, 2997, 2193, 3468, 2360], ['traffic light', 0.5544954538345337, 2753, 1024, 2889, 1246], ['car', 0.5168837308883667, 3646, 2242, 3771, 2365], ['car', 0.46331116557121277, 1382, 2156, 1514, 2258], ['person', 0.45071884989738464, 2219, 2174, 2261, 2290], ['car', 0.43537411093711853, 1300, 2155, 1401, 2255], ['truck', 0.4348198473453522, 2993, 2192, 3465, 2360], ['person', 0.4261070191860199, 2003, 2150, 2063, 2300], ['person', 0.39044731855392456, 2303, 2141, 2352, 2297], ['traffic light', 0.3781171441078186, 3473, 1414, 3716, 1612], ['person', 0.3775831162929535, 2649, 2169, 2706, 2315], ['car', 0.28386202454566956, 1301, 2154, 1521, 2259], ['traffic light', 0.27545157074928284, 540, 1571, 606, 1699], ['person', 0.2723023593425751, 2160, 2168, 2206, 2291], ['traffic light', 0.22903627157211304, 2963, 1034, 3178, 1224]], 0.1227719783782959)\n"
     ]
    }
   ],
   "source": [
    "gpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
    "print(gpu_openvino_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "\n",
      "infer time: 8.8292966 s\n",
      "infer time: 0.0243259000000009 s\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 26.7ms\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image_3.jpg: 640x640 6 persons, 8 cars, 1 truck, 10 traffic lights, 26.7ms\n",
      "Speed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 3.0ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Elapsed time: 0.0267 seconds\n",
      "Class: car, Confidence: 0.93, Box: [1, 2018, 663, 2354]\n",
      "Class: car, Confidence: 0.91, Box: [815, 2097, 1315, 2295]\n",
      "Class: person, Confidence: 0.82, Box: [1487, 2043, 1569, 2323]\n",
      "Class: traffic light, Confidence: 0.82, Box: [1346, 1262, 1409, 1428]\n",
      "Class: traffic light, Confidence: 0.81, Box: [1907, 1279, 1967, 1447]\n",
      "Class: traffic light, Confidence: 0.81, Box: [1162, 1202, 1228, 1366]\n",
      "Class: traffic light, Confidence: 0.80, Box: [3014, 773, 3155, 1041]\n",
      "Class: traffic light, Confidence: 0.76, Box: [629, 1187, 698, 1353]\n",
      "Class: car, Confidence: 0.71, Box: [1799, 2216, 1914, 2290]\n",
      "Class: traffic light, Confidence: 0.67, Box: [411, 1513, 488, 1638]\n",
      "Class: car, Confidence: 0.63, Box: [2997, 2193, 3468, 2360]\n",
      "Class: traffic light, Confidence: 0.55, Box: [2753, 1024, 2889, 1246]\n",
      "Class: car, Confidence: 0.52, Box: [3646, 2242, 3771, 2365]\n",
      "Class: car, Confidence: 0.46, Box: [1382, 2156, 1514, 2258]\n",
      "Class: person, Confidence: 0.45, Box: [2219, 2174, 2261, 2290]\n",
      "Class: car, Confidence: 0.44, Box: [1300, 2155, 1401, 2255]\n",
      "Class: truck, Confidence: 0.43, Box: [2993, 2192, 3465, 2360]\n",
      "Class: person, Confidence: 0.43, Box: [2003, 2150, 2063, 2300]\n",
      "Class: person, Confidence: 0.39, Box: [2303, 2141, 2352, 2297]\n",
      "Class: traffic light, Confidence: 0.38, Box: [3473, 1414, 3716, 1612]\n",
      "Class: person, Confidence: 0.38, Box: [2649, 2169, 2706, 2315]\n",
      "Class: car, Confidence: 0.28, Box: [1301, 2154, 1521, 2259]\n",
      "Class: traffic light, Confidence: 0.28, Box: [540, 1571, 606, 1699]\n",
      "Class: person, Confidence: 0.27, Box: [2160, 2168, 2206, 2291]\n",
      "Class: traffic light, Confidence: 0.23, Box: [2963, 1034, 3178, 1224]\n",
      "([['car', 0.9287983775138855, 1, 2018, 663, 2354], ['car', 0.907758891582489, 815, 2097, 1315, 2295], ['person', 0.8231251835823059, 1487, 2043, 1569, 2323], ['traffic light', 0.8191970586776733, 1346, 1262, 1409, 1428], ['traffic light', 0.8144829273223877, 1907, 1279, 1967, 1447], ['traffic light', 0.8084127306938171, 1162, 1202, 1228, 1366], ['traffic light', 0.8003895878791809, 3014, 773, 3155, 1041], ['traffic light', 0.7566081285476685, 629, 1187, 698, 1353], ['car', 0.7073069214820862, 1799, 2216, 1914, 2290], ['traffic light', 0.6712331771850586, 411, 1513, 488, 1638], ['car', 0.6344873309135437, 2997, 2193, 3468, 2360], ['traffic light', 0.5543507933616638, 2753, 1024, 2889, 1246], ['car', 0.5169419646263123, 3646, 2242, 3771, 2365], ['car', 0.4632578492164612, 1382, 2156, 1514, 2258], ['person', 0.45093804597854614, 2219, 2174, 2261, 2290], ['car', 0.43534231185913086, 1300, 2155, 1401, 2255], ['truck', 0.4344015121459961, 2993, 2192, 3465, 2360], ['person', 0.42606961727142334, 2003, 2150, 2063, 2300], ['person', 0.39045780897140503, 2303, 2141, 2352, 2297], ['traffic light', 0.37784838676452637, 3473, 1414, 3716, 1612], ['person', 0.3765757083892822, 2649, 2169, 2706, 2315], ['car', 0.28381168842315674, 1301, 2154, 1521, 2259], ['traffic light', 0.27551716566085815, 540, 1571, 606, 1699], ['person', 0.2722732424736023, 2160, 2168, 2206, 2291], ['traffic light', 0.22981375455856323, 2963, 1034, 3178, 1224]], 0.026744365692138672)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
    "print(gpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "run infer time: 8.477141500000002\n",
      "Elapsed time: 8.5435 seconds\n",
      "Class: car, Confidence: 0.94, Box: [0, 2016, 659, 2356]\n",
      "Class: car, Confidence: 0.90, Box: [815, 2100, 1316, 2297]\n",
      "Class: person, Confidence: 0.84, Box: [1488, 2047, 1568, 2320]\n",
      "Class: traffic light, Confidence: 0.84, Box: [1346, 1261, 1408, 1429]\n",
      "Class: traffic light, Confidence: 0.84, Box: [1907, 1281, 1967, 1447]\n",
      "Class: traffic light, Confidence: 0.82, Box: [1160, 1203, 1228, 1368]\n",
      "Class: car, Confidence: 0.78, Box: [1795, 2214, 1917, 2288]\n",
      "Class: traffic light, Confidence: 0.76, Box: [3015, 773, 3161, 1041]\n",
      "Class: traffic light, Confidence: 0.71, Box: [628, 1188, 699, 1355]\n",
      "Class: traffic light, Confidence: 0.68, Box: [539, 1578, 602, 1692]\n",
      "Class: traffic light, Confidence: 0.68, Box: [2751, 1027, 2872, 1242]\n",
      "Class: car, Confidence: 0.66, Box: [3643, 2222, 4029, 2373]\n",
      "Class: traffic light, Confidence: 0.63, Box: [411, 1513, 487, 1630]\n",
      "Class: person, Confidence: 0.61, Box: [2209, 2169, 2266, 2290]\n",
      "Class: person, Confidence: 0.60, Box: [2647, 2175, 2716, 2314]\n",
      "Class: truck, Confidence: 0.53, Box: [2996, 2193, 3470, 2356]\n",
      "Class: car, Confidence: 0.52, Box: [1297, 2159, 1410, 2256]\n",
      "Class: traffic light, Confidence: 0.43, Box: [3472, 1418, 3716, 1608]\n",
      "Class: car, Confidence: 0.37, Box: [1375, 2155, 1526, 2261]\n",
      "Class: person, Confidence: 0.35, Box: [2155, 2158, 2205, 2289]\n",
      "Class: person, Confidence: 0.32, Box: [2300, 2143, 2357, 2295]\n",
      "Class: car, Confidence: 0.27, Box: [1369, 2154, 1658, 2264]\n",
      "Class: car, Confidence: 0.27, Box: [1696, 2193, 1796, 2234]\n",
      "Class: person, Confidence: 0.26, Box: [2401, 2210, 2475, 2314]\n",
      "Class: car, Confidence: 0.23, Box: [1304, 2158, 1454, 2257]\n",
      "Class: car, Confidence: 0.20, Box: [3644, 2242, 3768, 2365]\n",
      "([['car', 0.9441971182823181, 0, 2016, 659, 2356], ['car', 0.9048448204994202, 815, 2100, 1316, 2297], ['person', 0.8449991345405579, 1488, 2047, 1568, 2320], ['traffic light', 0.8443686366081238, 1346, 1261, 1408, 1429], ['traffic light', 0.8419590592384338, 1907, 1281, 1967, 1447], ['traffic light', 0.817879319190979, 1160, 1203, 1228, 1368], ['car', 0.777797520160675, 1795, 2214, 1917, 2288], ['traffic light', 0.7590904235839844, 3015, 773, 3161, 1041], ['traffic light', 0.7135507464408875, 628, 1188, 699, 1355], ['traffic light', 0.6808620095252991, 539, 1578, 602, 1692], ['traffic light', 0.6769329905509949, 2751, 1027, 2872, 1242], ['car', 0.6552550792694092, 3643, 2222, 4029, 2373], ['traffic light', 0.6328028440475464, 411, 1513, 487, 1630], ['person', 0.6065565943717957, 2209, 2169, 2266, 2290], ['person', 0.5959166884422302, 2647, 2175, 2716, 2314], ['truck', 0.5270811915397644, 2996, 2193, 3470, 2356], ['car', 0.5184987783432007, 1297, 2159, 1410, 2256], ['traffic light', 0.42876702547073364, 3472, 1418, 3716, 1608], ['car', 0.3728940486907959, 1375, 2155, 1526, 2261], ['person', 0.35390084981918335, 2155, 2158, 2205, 2289], ['person', 0.32359135150909424, 2300, 2143, 2357, 2295], ['car', 0.27048587799072266, 1369, 2154, 1658, 2264], ['car', 0.2671922445297241, 1696, 2193, 1796, 2234], ['person', 0.2572038173675537, 2401, 2210, 2475, 2314], ['car', 0.22689151763916016, 1304, 2158, 1454, 2257], ['car', 0.2044004201889038, 3644, 2242, 3768, 2365]], 8.543483800000004)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
    "print(gpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"gpu conf_diff\", \"gpu box_diff (px)\"])\n",
    "\n",
    "df_pt_openvino = generate_difference_df(gpu_pytorch_image[0], gpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
    "df_pt_onnx = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
    "df_pt_onnxruntime = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average GPU Time (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "pytorch_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_pytorch)\n",
    "openvino_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_openvino)\n",
    "onnx_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_onnx)\n",
    "onnx_runtime_func = partial(yolo_runtime_test.onnxruntime_run_image, args=args_onnx)\n",
    "\n",
    "result_time.append(collect_execution_times(pytorch_func))\n",
    "result_time.append(collect_execution_times(openvino_func))\n",
    "result_time.append(collect_execution_times(onnx_func))\n",
    "result_time.append(collect_execution_times(onnx_runtime_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time gpu (ms)\",\n",
    "                           \"openvino+ultralytics time gpu (ms)\",\n",
    "                           \"onnxâ€‹+ultralytics time gpu (ms)\", \n",
    "                           \"onnx runtime time gpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save GPU result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./app/saved_pkl/gpu_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from app.util.timer import Timer\n",
    "from app.util.util import Differ\n",
    "from main import YoloRuntimeTest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch = {\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\",\n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}\n",
    "\n",
    "args_onnx = {\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}\n",
    "\n",
    "args_openvino = {\n",
    "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize YOLO runtime test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_runtime_test = YoloRuntimeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 95.7ms\n",
      "Speed: 4.5ms preprocess, 95.7ms inference, 997.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 2.0204 seconds\n",
      "([['car', 0.9298826456069946, 558, 206, 808, 359], ['car', 0.9204895496368408, 286, 210, 458, 352], ['car', 0.9116721153259277, 465, 217, 596, 339], ['person', 0.8727421164512634, 159, 143, 301, 403], ['truck', 0.8677256107330322, 103, 90, 255, 316], ['truck', 0.779636561870575, 722, 170, 871, 346], ['truck', 0.7476049661636353, 0, 154, 94, 354], ['bicycle', 0.6518727540969849, 210, 321, 266, 443], ['car', 0.5187947154045105, 78, 212, 113, 300], ['car', 0.36107298731803894, 420, 226, 474, 319], ['car', 0.2998500466346741, 420, 227, 464, 278]], 2.0203989000001457)\n"
     ]
    }
   ],
   "source": [
    "gpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
    "print(gpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 142.8ms\n",
      "Speed: 4.0ms preprocess, 142.8ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.9338 seconds\n",
      "([['car', 0.9305108189582825, 558, 206, 808, 359], ['car', 0.9209165573120117, 286, 210, 458, 352], ['car', 0.9119921326637268, 465, 217, 596, 339], ['person', 0.8731657266616821, 159, 143, 301, 403], ['truck', 0.868000864982605, 103, 89, 255, 316], ['truck', 0.7941579818725586, 722, 170, 871, 346], ['truck', 0.74634850025177, 0, 154, 94, 354], ['bicycle', 0.6512935757637024, 210, 321, 266, 443], ['car', 0.5251946449279785, 78, 212, 113, 300], ['car', 0.3632618486881256, 420, 226, 474, 319], ['car', 0.2982390820980072, 420, 227, 464, 278], ['traffic light', 0.21935202181339264, 258, 82, 274, 116]], 0.9338370999998915)\n"
     ]
    }
   ],
   "source": [
    "gpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
    "print(gpu_openvino_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 9.8861 seconds\n",
      "([['car', 0.9304918050765991, 558, 206, 808, 359], ['car', 0.9209156632423401, 286, 210, 458, 352], ['car', 0.9119675159454346, 465, 217, 596, 339], ['person', 0.8731945753097534, 159, 143, 301, 403], ['truck', 0.8679496049880981, 103, 89, 255, 316], ['truck', 0.7939761877059937, 722, 170, 871, 346], ['truck', 0.7464209198951721, 0, 154, 94, 354], ['bicycle', 0.6515001654624939, 210, 321, 266, 443], ['car', 0.5250102281570435, 78, 212, 113, 300], ['car', 0.3633507490158081, 420, 226, 474, 319], ['car', 0.29814159870147705, 420, 227, 464, 278], ['traffic light', 0.21931332349777222, 258, 82, 274, 116]], 9.886120699999992)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
    "print(gpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.9648 seconds\n",
      "([['car', 0.9232282638549805, 556, 206, 810, 359], ['car', 0.916882336139679, 463, 217, 595, 338], ['car', 0.9103444218635559, 286, 210, 459, 351], ['person', 0.9022980332374573, 159, 143, 299, 403], ['truck', 0.8684875965118408, 723, 171, 871, 345], ['truck', 0.8390284180641174, 102, 89, 257, 314], ['bicycle', 0.7249286770820618, 209, 322, 269, 441], ['truck', 0.6843006610870361, 0, 154, 93, 354], ['car', 0.3992183804512024, 78, 225, 113, 300], ['car', 0.2260175347328186, 421, 225, 483, 268], ['car', 0.21741265058517456, 421, 229, 470, 320]], 8.96478470000011)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
    "print(gpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pt vs openvino+ultralytics gpu conf_diff     gpu box_diff (px) pt vs onnx+ultralytics gpu conf_diff     gpu box_diff (px) pt vs onnxruntime gpu conf_diff       gpu box_diff (px)\n",
      "0                         car        0.0006  [0.0, 0.0, 0.0, 0.0]                    car        0.0006  [0.0, 0.0, 0.0, 0.0]               car        0.0067    [2.0, 0.0, 2.0, 0.0]\n",
      "1                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0101    [0.0, 0.0, 1.0, 1.0]\n",
      "2                         car        0.0003  [0.0, 0.0, 0.0, 0.0]                    car        0.0003  [0.0, 0.0, 0.0, 0.0]               car        0.0052    [2.0, 0.0, 1.0, 1.0]\n",
      "3                      person        0.0004  [0.0, 0.0, 0.0, 0.0]                 person        0.0005  [0.0, 0.0, 0.0, 0.0]            person        0.0296    [0.0, 0.0, 2.0, 0.0]\n",
      "4                       truck        0.0003  [0.0, 1.0, 0.0, 0.0]                  truck        0.0002  [0.0, 1.0, 0.0, 0.0]             truck        0.0287    [1.0, 1.0, 2.0, 2.0]\n",
      "5                       truck        0.0145  [0.0, 0.0, 0.0, 0.0]                  truck        0.0143  [0.0, 0.0, 0.0, 0.0]             truck        0.0889    [1.0, 1.0, 0.0, 1.0]\n",
      "6                       truck        0.0013  [0.0, 0.0, 0.0, 0.0]                  truck        0.0012  [0.0, 0.0, 0.0, 0.0]             truck        0.0633    [0.0, 0.0, 1.0, 0.0]\n",
      "7                     bicycle        0.0006  [0.0, 0.0, 0.0, 0.0]                bicycle        0.0004  [0.0, 0.0, 0.0, 0.0]           bicycle        0.0731    [1.0, 1.0, 3.0, 2.0]\n",
      "8                         car        0.0064  [0.0, 0.0, 0.0, 0.0]                    car        0.0062  [0.0, 0.0, 0.0, 0.0]               car        0.1196   [0.0, 13.0, 0.0, 0.0]\n",
      "9                         car        0.0022  [0.0, 0.0, 0.0, 0.0]                    car        0.0023  [0.0, 0.0, 0.0, 0.0]               car        0.1437    [1.0, 3.0, 4.0, 1.0]\n",
      "10                        car        0.0016  [0.0, 0.0, 0.0, 0.0]                    car        0.0017  [0.0, 0.0, 0.0, 0.0]               car        0.0738  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"gpu conf_diff\", \"gpu box_diff (px)\"])\n",
    "\n",
    "df_pt_openvino = generate_difference_df(gpu_pytorch_image[0], gpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
    "df_pt_onnx = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
    "df_pt_onnxruntime = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average GPU Time (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1895 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1780 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.6ms\n",
      "Speed: 1.0ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1814 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 8.5ms\n",
      "Speed: 1.0ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1946 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.1ms\n",
      "Speed: 1.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1988 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1962 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1913 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.5ms\n",
      "Speed: 1.0ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1986 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1968 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.1ms\n",
      "Speed: 1.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1928 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 131.8ms\n",
      "Speed: 4.5ms preprocess, 131.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5530 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 131.8ms\n",
      "Speed: 4.0ms preprocess, 131.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5003 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.0ms\n",
      "Speed: 5.0ms preprocess, 128.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5476 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 124.6ms\n",
      "Speed: 6.0ms preprocess, 124.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5418 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 127.3ms\n",
      "Speed: 5.5ms preprocess, 127.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5426 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 152.0ms\n",
      "Speed: 4.0ms preprocess, 152.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5734 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 124.8ms\n",
      "Speed: 5.5ms preprocess, 124.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5399 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 129.7ms\n",
      "Speed: 4.6ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5433 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 124.1ms\n",
      "Speed: 3.0ms preprocess, 124.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5329 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 123.3ms\n",
      "Speed: 5.5ms preprocess, 123.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5489 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 9.2693 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 25.6ms\n",
      "Speed: 5.0ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 9.1867 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 19.6ms\n",
      "Speed: 2.0ms preprocess, 19.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 9.0137 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.5513 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.5564 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 25.4ms\n",
      "Speed: 3.0ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.5695 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.9ms\n",
      "Speed: 1.5ms preprocess, 20.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.5221 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.6180 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.2ms\n",
      "Speed: 2.0ms preprocess, 20.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.2889 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 21.9ms\n",
      "Speed: 2.0ms preprocess, 21.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.9875 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8091 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8018 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.7995 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.7922 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8241 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.73, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.7970 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8257 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8013 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.8150 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.7928 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "pytorch_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_pytorch)\n",
    "openvino_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_openvino)\n",
    "onnx_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_onnx)\n",
    "onnx_runtime_func = partial(yolo_runtime_test.onnxruntime_run_image, args=args_onnx)\n",
    "\n",
    "result_time.append(collect_execution_times(pytorch_func))\n",
    "result_time.append(collect_execution_times(openvino_func))\n",
    "result_time.append(collect_execution_times(onnx_func))\n",
    "result_time.append(collect_execution_times(onnx_runtime_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time gpu (ms)</th>\n",
       "      <th>openvino+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx runtime time gpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.180150</td>\n",
       "      <td>54.237466</td>\n",
       "      <td>755.634100</td>\n",
       "      <td>880.584685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707385</td>\n",
       "      <td>1.828821</td>\n",
       "      <td>134.596865</td>\n",
       "      <td>1.215965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.803730</td>\n",
       "      <td>50.033740</td>\n",
       "      <td>628.893110</td>\n",
       "      <td>879.220870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.372015</td>\n",
       "      <td>54.296100</td>\n",
       "      <td>659.374800</td>\n",
       "      <td>880.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>19.859289</td>\n",
       "      <td>55.502139</td>\n",
       "      <td>919.496458</td>\n",
       "      <td>882.424505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>19.869139</td>\n",
       "      <td>56.422475</td>\n",
       "      <td>923.213314</td>\n",
       "      <td>882.496077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.878990</td>\n",
       "      <td>57.342810</td>\n",
       "      <td>926.930170</td>\n",
       "      <td>882.567650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time gpu (ms)  openvino+ultralytics time gpu (ms)  onnx+ultralytics time gpu (ms)  onnx runtime time gpu (ms)\n",
       "count              10.000000                           10.000000                        10.000000                   10.000000\n",
       "mean               19.180150                           54.237466                       755.634100                  880.584685\n",
       "std                 0.707385                            1.828821                       134.596865                    1.215965\n",
       "min                17.803730                           50.033740                       628.893110                  879.220870\n",
       "50%                19.372015                           54.296100                       659.374800                  880.151500\n",
       "90%                19.859289                           55.502139                       919.496458                  882.424505\n",
       "95%                19.869139                           56.422475                       923.213314                  882.496077\n",
       "max                19.878990                           57.342810                       926.930170                  882.567650"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time gpu (ms)\",\n",
    "                           \"openvino+ultralytics time gpu (ms)\",\n",
    "                           \"onnx+ultralytics time gpu (ms)\", \n",
    "                           \"onnx runtime time gpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save GPU result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./app/saved_pkl/gpu_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

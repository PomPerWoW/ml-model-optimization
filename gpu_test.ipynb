{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from app.util.timer import Timer\n",
    "from app.util.util import Differ\n",
    "from main import YoloRuntimeTest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch = {\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\",\n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}\n",
    "\n",
    "args_onnx = {\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}\n",
    "\n",
    "args_openvino = {\n",
    "    \"weights\": \"./app/weights/yolov9c_openvino_model\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\",\n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize YOLO runtime test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_runtime_test = YoloRuntimeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 96.5ms\n",
      "Speed: 3.4ms preprocess, 96.5ms inference, 507.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.1042 seconds\n",
      "([['car', 0.9298826456069946, 558, 206, 808, 359], ['car', 0.9204895496368408, 286, 210, 458, 352], ['car', 0.9116721153259277, 465, 217, 596, 339], ['person', 0.8727421164512634, 159, 143, 301, 403], ['truck', 0.8677256107330322, 103, 90, 255, 316], ['truck', 0.779636561870575, 722, 170, 871, 346], ['truck', 0.7476049661636353, 0, 154, 94, 354], ['bicycle', 0.6518727540969849, 210, 321, 266, 443], ['car', 0.5187947154045105, 78, 212, 113, 300], ['car', 0.36107298731803894, 420, 226, 474, 319], ['car', 0.2998500466346741, 420, 227, 464, 278]], 1.1041930000000004)\n"
     ]
    }
   ],
   "source": [
    "gpu_pytorch_image = yolo_runtime_test.ultralytics_run_image(args_pytorch)\n",
    "print(gpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 128.4ms\n",
      "Speed: 2.0ms preprocess, 128.4ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4525 seconds\n",
      "([['car', 0.9305108189582825, 558, 206, 808, 359], ['car', 0.9209165573120117, 286, 210, 458, 352], ['car', 0.9119921326637268, 465, 217, 596, 339], ['person', 0.8731657266616821, 159, 143, 301, 403], ['truck', 0.868000864982605, 103, 89, 255, 316], ['truck', 0.7941579818725586, 722, 170, 871, 346], ['truck', 0.74634850025177, 0, 154, 94, 354], ['bicycle', 0.6512935757637024, 210, 321, 266, 443], ['car', 0.5251946449279785, 78, 212, 113, 300], ['car', 0.3632618486881256, 420, 226, 474, 319], ['car', 0.2982390820980072, 420, 227, 464, 278], ['traffic light', 0.21935202181339264, 258, 82, 274, 116]], 0.45248449999999885)\n"
     ]
    }
   ],
   "source": [
    "gpu_openvino_image = yolo_runtime_test.ultralytics_run_image(args_openvino)\n",
    "print(gpu_openvino_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.7470 seconds\n",
      "([['car', 0.930499792098999, 558, 206, 808, 359], ['car', 0.9209237694740295, 286, 210, 458, 352], ['car', 0.9119822382926941, 465, 217, 596, 339], ['person', 0.8731963634490967, 159, 143, 301, 403], ['truck', 0.867991030216217, 103, 89, 255, 316], ['truck', 0.7941445112228394, 722, 170, 871, 346], ['truck', 0.7465292811393738, 0, 154, 94, 354], ['bicycle', 0.651324987411499, 210, 321, 266, 443], ['car', 0.5250387191772461, 78, 212, 113, 300], ['car', 0.36375463008880615, 420, 226, 474, 319], ['car', 0.2979387044906616, 420, 227, 464, 278], ['traffic light', 0.21963763236999512, 258, 82, 274, 116]], 8.7470099)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_image = yolo_runtime_test.ultralytics_run_image(args_onnx)\n",
    "print(gpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.5365 seconds\n",
      "([['car', 0.9232661724090576, 556, 206, 810, 359], ['car', 0.9168801307678223, 463, 217, 595, 338], ['car', 0.9103737473487854, 286, 210, 459, 351], ['person', 0.9022603631019592, 159, 143, 299, 403], ['truck', 0.8684412837028503, 723, 171, 871, 345], ['truck', 0.8390982747077942, 102, 89, 257, 314], ['bicycle', 0.7248467206954956, 209, 322, 269, 441], ['truck', 0.6843962669372559, 0, 154, 93, 354], ['car', 0.39986860752105713, 78, 225, 113, 300], ['car', 0.22589325904846191, 421, 225, 483, 268], ['car', 0.2174329161643982, 421, 229, 470, 320]], 8.5365215)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_runtime_image = yolo_runtime_test.onnxruntime_run_image(args_onnx)\n",
    "print(gpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pt vs openvino+ultralytics gpu conf_diff     gpu box_diff (px) pt vs onnx+ultralytics gpu conf_diff     gpu box_diff (px) pt vs onnxruntime gpu conf_diff       gpu box_diff (px)\n",
      "0                         car        0.0006  [0.0, 0.0, 0.0, 0.0]                    car        0.0006  [0.0, 0.0, 0.0, 0.0]               car        0.0066    [2.0, 0.0, 2.0, 0.0]\n",
      "1                         car        0.0004  [0.0, 0.0, 0.0, 0.0]                    car        0.0004  [0.0, 0.0, 0.0, 0.0]               car        0.0101    [0.0, 0.0, 1.0, 1.0]\n",
      "2                         car        0.0003  [0.0, 0.0, 0.0, 0.0]                    car        0.0003  [0.0, 0.0, 0.0, 0.0]               car        0.0052    [2.0, 0.0, 1.0, 1.0]\n",
      "3                      person        0.0004  [0.0, 0.0, 0.0, 0.0]                 person        0.0005  [0.0, 0.0, 0.0, 0.0]            person        0.0295    [0.0, 0.0, 2.0, 0.0]\n",
      "4                       truck        0.0003  [0.0, 1.0, 0.0, 0.0]                  truck        0.0003  [0.0, 1.0, 0.0, 0.0]             truck        0.0286    [1.0, 1.0, 2.0, 2.0]\n",
      "5                       truck        0.0145  [0.0, 0.0, 0.0, 0.0]                  truck        0.0145  [0.0, 0.0, 0.0, 0.0]             truck        0.0888    [1.0, 1.0, 0.0, 1.0]\n",
      "6                       truck        0.0013  [0.0, 0.0, 0.0, 0.0]                  truck        0.0011  [0.0, 0.0, 0.0, 0.0]             truck        0.0632    [0.0, 0.0, 1.0, 0.0]\n",
      "7                     bicycle        0.0006  [0.0, 0.0, 0.0, 0.0]                bicycle        0.0005  [0.0, 0.0, 0.0, 0.0]           bicycle        0.0730    [1.0, 1.0, 3.0, 2.0]\n",
      "8                         car        0.0064  [0.0, 0.0, 0.0, 0.0]                    car        0.0062  [0.0, 0.0, 0.0, 0.0]               car        0.1189   [0.0, 13.0, 0.0, 0.0]\n",
      "9                         car        0.0022  [0.0, 0.0, 0.0, 0.0]                    car        0.0027  [0.0, 0.0, 0.0, 0.0]               car        0.1436    [1.0, 3.0, 4.0, 1.0]\n",
      "10                        car        0.0016  [0.0, 0.0, 0.0, 0.0]                    car        0.0019  [0.0, 0.0, 0.0, 0.0]               car        0.0740  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"gpu conf_diff\", \"gpu box_diff (px)\"])\n",
    "\n",
    "df_pt_openvino = generate_difference_df(gpu_pytorch_image[0], gpu_openvino_image[0], \"pt vs openvino+ultralytics\")\n",
    "df_pt_onnx = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_image[0], \"pt vs onnx+ultralytics\")\n",
    "df_pt_onnxruntime = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_runtime_image[0], \"pt vs onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_openvino, df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average GPU Time (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1720 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.2ms\n",
      "Speed: 1.0ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2095 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.5ms\n",
      "Speed: 1.0ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1875 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 10.5ms\n",
      "Speed: 1.2ms preprocess, 10.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1975 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 12.5ms\n",
      "Speed: 1.0ms preprocess, 12.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2119 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 10.5ms\n",
      "Speed: 1.0ms preprocess, 10.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1977 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 12.1ms\n",
      "Speed: 1.0ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1970 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 13.0ms\n",
      "Speed: 0.5ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2021 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 12.6ms\n",
      "Speed: 1.0ms preprocess, 12.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1957 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1804 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 119.6ms\n",
      "Speed: 2.0ms preprocess, 119.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4408 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 119.2ms\n",
      "Speed: 1.0ms preprocess, 119.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4094 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 119.8ms\n",
      "Speed: 2.0ms preprocess, 119.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4059 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 115.6ms\n",
      "Speed: 1.5ms preprocess, 115.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4202 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 115.2ms\n",
      "Speed: 1.0ms preprocess, 115.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4043 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 114.5ms\n",
      "Speed: 3.0ms preprocess, 114.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4049 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 121.1ms\n",
      "Speed: 3.0ms preprocess, 121.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.5550 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 116.4ms\n",
      "Speed: 1.5ms preprocess, 116.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.3964 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 117.2ms\n",
      "Speed: 2.0ms preprocess, 117.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4153 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 114.1ms\n",
      "Speed: 2.0ms preprocess, 114.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 0.4128 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.7696 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.6505 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.6721 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.2946 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.3315 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 25.0ms\n",
      "Speed: 2.5ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.3339 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 24.3ms\n",
      "Speed: 3.0ms preprocess, 24.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.3122 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.0ms\n",
      "Speed: 1.0ms preprocess, 20.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.3182 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 20.0ms\n",
      "Speed: 1.5ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 6.0782 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 c:\\Users\\User\\OneDrive\\Desktop\\pomper\\Internship\\yolov9\\app\\assets\\sample_image.jpeg: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 1 traffic light, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Class: traffic light, Confidence: 0.22, Box: [258, 82, 274, 116]\n",
      "Elapsed time: 8.6173 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4443 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4543 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4549 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4550 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4509 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4595 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4468 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4375 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4186 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 8.4259 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "pytorch_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_pytorch)\n",
    "openvino_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_openvino)\n",
    "onnx_func = partial(yolo_runtime_test.ultralytics_run_image, args=args_onnx)\n",
    "onnx_runtime_func = partial(yolo_runtime_test.onnxruntime_run_image, args=args_onnx)\n",
    "\n",
    "result_time.append(collect_execution_times(pytorch_func))\n",
    "result_time.append(collect_execution_times(openvino_func))\n",
    "result_time.append(collect_execution_times(onnx_func))\n",
    "result_time.append(collect_execution_times(onnx_runtime_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time gpu (ms)</th>\n",
       "      <th>openvino+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx runtime time gpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.512315</td>\n",
       "      <td>42.650696</td>\n",
       "      <td>723.781164</td>\n",
       "      <td>844.477048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.231816</td>\n",
       "      <td>4.672341</td>\n",
       "      <td>124.174836</td>\n",
       "      <td>1.352624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.197640</td>\n",
       "      <td>39.640820</td>\n",
       "      <td>607.818180</td>\n",
       "      <td>841.863840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.722870</td>\n",
       "      <td>41.111000</td>\n",
       "      <td>633.269640</td>\n",
       "      <td>844.884340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>20.977700</td>\n",
       "      <td>45.224911</td>\n",
       "      <td>868.183654</td>\n",
       "      <td>845.542096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>21.085475</td>\n",
       "      <td>50.362251</td>\n",
       "      <td>872.569777</td>\n",
       "      <td>845.744803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.193250</td>\n",
       "      <td>55.499590</td>\n",
       "      <td>876.955900</td>\n",
       "      <td>845.947510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time gpu (ms)  openvino+ultralytics time gpu (ms)  onnx+ultralytics time gpu (ms)  onnx runtime time gpu (ms)\n",
       "count              10.000000                           10.000000                        10.000000                   10.000000\n",
       "mean               19.512315                           42.650696                       723.781164                  844.477048\n",
       "std                 1.231816                            4.672341                       124.174836                    1.352624\n",
       "min                17.197640                           39.640820                       607.818180                  841.863840\n",
       "50%                19.722870                           41.111000                       633.269640                  844.884340\n",
       "90%                20.977700                           45.224911                       868.183654                  845.542096\n",
       "95%                21.085475                           50.362251                       872.569777                  845.744803\n",
       "max                21.193250                           55.499590                       876.955900                  845.947510"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time gpu (ms)\",\n",
    "                           \"openvino+ultralytics time gpu (ms)\",\n",
    "                           \"onnx+ultralytics time gpu (ms)\", \n",
    "                           \"onnx runtime time gpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

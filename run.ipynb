{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from timer import Timer\n",
    "from app.util.util import DotDict, Differ\n",
    "from main import yolo_run_image, yolo_run_video, inference_on_image, inference_on_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_pytorch_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_onnx_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_onnx_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "})\n",
    "\n",
    "args_pytorch_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "})\n",
    "\n",
    "args_onnx_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "})\n",
    "\n",
    "args_onnx_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cuda:1\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pytorch_image():\n",
    "    result = yolo_run_image(args_pytorch_image)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_image():\n",
    "    result = yolo_run_image(args_onnx_image)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_runtime_image():\n",
    "    result = inference_on_image(args_onnx_image)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pytorch_video():\n",
    "    result = yolo_run_video(args_pytorch_video)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_video():\n",
    "    result = yolo_run_video(args_onnx_video)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_runtime_video():\n",
    "    result = inference_on_video(args_onnx_video)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 121.5ms\n",
      "Speed: 1.0ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6053 seconds\n",
      "([['car', 0.9299132823944092, 558, 206, 808, 359], ['car', 0.9205407500267029, 286, 210, 458, 352], ['car', 0.9116201996803284, 465, 217, 596, 339], ['person', 0.8726341724395752, 159, 143, 301, 403], ['truck', 0.8677411079406738, 103, 90, 255, 316], ['truck', 0.7794504761695862, 722, 170, 871, 346], ['truck', 0.7472285628318787, 0, 154, 94, 354], ['bicycle', 0.651627779006958, 210, 321, 266, 443], ['car', 0.5186470150947571, 78, 212, 113, 300], ['car', 0.36083880066871643, 420, 226, 474, 319], ['car', 0.3002343475818634, 420, 227, 464, 278]], 1.605288900000005)\n"
     ]
    }
   ],
   "source": [
    "pytorch_image = run_inference_pytorch_image()\n",
    "print(pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 122.9ms\n",
      "Speed: 1.0ms preprocess, 122.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6538 seconds\n",
      "([['car', 0.9305105805397034, 558, 206, 808, 359], ['car', 0.9209166169166565, 286, 210, 458, 352], ['car', 0.9119920134544373, 465, 217, 596, 339], ['person', 0.8731658458709717, 159, 143, 301, 403], ['truck', 0.8680006265640259, 103, 89, 255, 316], ['truck', 0.7941577434539795, 722, 170, 871, 346], ['truck', 0.7463480830192566, 0, 154, 94, 354], ['bicycle', 0.6512946486473083, 210, 321, 266, 443], ['car', 0.5251938104629517, 78, 212, 113, 300], ['car', 0.36326032876968384, 420, 226, 474, 319], ['car', 0.2982402443885803, 420, 227, 464, 278]], 1.653805900000009)\n"
     ]
    }
   ],
   "source": [
    "onnx_image = run_inference_onnx_image()\n",
    "print(onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1262 seconds\n",
      "([['car', 0.9232816696166992, 556, 206, 810, 359], ['car', 0.916867733001709, 463, 217, 595, 338], ['car', 0.9103224277496338, 286, 210, 459, 351], ['person', 0.9022834300994873, 159, 143, 299, 403], ['truck', 0.8684066534042358, 723, 171, 871, 345], ['truck', 0.8390960693359375, 102, 89, 257, 314], ['bicycle', 0.724895179271698, 209, 322, 269, 441], ['truck', 0.6845371723175049, 0, 154, 93, 354], ['car', 0.39963772892951965, 78, 225, 113, 300], ['car', 0.22634947299957275, 421, 225, 483, 268], ['car', 0.21756017208099365, 421, 229, 470, 320]], 0.12615449999998418)\n"
     ]
    }
   ],
   "source": [
    "onnx_runtime_image = run_inference_onnx_runtime_image()\n",
    "print(onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 14.5ms\n",
      "Speed: 2.5ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1989 seconds\n",
      "([['car', 0.9298826456069946, 558, 206, 808, 359], ['car', 0.9204895496368408, 286, 210, 458, 352], ['car', 0.9116721153259277, 465, 217, 596, 339], ['person', 0.8727421164512634, 159, 143, 301, 403], ['truck', 0.8677256107330322, 103, 90, 255, 316], ['truck', 0.779636561870575, 722, 170, 871, 346], ['truck', 0.7476049661636353, 0, 154, 94, 354], ['bicycle', 0.6518727540969849, 210, 321, 266, 443], ['car', 0.5187947154045105, 78, 212, 113, 300], ['car', 0.36107298731803894, 420, 226, 474, 319], ['car', 0.2998500466346741, 420, 227, 464, 278]], 0.1988867999999968)\n"
     ]
    }
   ],
   "source": [
    "gpu_pytorch_image = run_inference_pytorch_image()\n",
    "print(gpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 124.0ms\n",
      "Speed: 2.0ms preprocess, 124.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5005 seconds\n",
      "([['car', 0.9305105805397034, 558, 206, 808, 359], ['car', 0.9209166765213013, 286, 210, 458, 352], ['car', 0.911992073059082, 465, 217, 596, 339], ['person', 0.8731656074523926, 159, 143, 301, 403], ['truck', 0.8680007457733154, 103, 89, 255, 316], ['truck', 0.794157862663269, 722, 170, 871, 346], ['truck', 0.74634850025177, 0, 154, 94, 354], ['bicycle', 0.6512946486473083, 210, 321, 266, 443], ['car', 0.5251935720443726, 78, 212, 113, 300], ['car', 0.36326080560684204, 420, 226, 474, 319], ['car', 0.2982398271560669, 420, 227, 464, 278]], 0.500492099999974)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_image = run_inference_onnx_image()\n",
    "print(gpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1260 seconds\n",
      "([['car', 0.9232816696166992, 556, 206, 810, 359], ['car', 0.916867733001709, 463, 217, 595, 338], ['car', 0.9103224277496338, 286, 210, 459, 351], ['person', 0.9022834300994873, 159, 143, 299, 403], ['truck', 0.8684066534042358, 723, 171, 871, 345], ['truck', 0.8390960693359375, 102, 89, 257, 314], ['bicycle', 0.724895179271698, 209, 322, 269, 441], ['truck', 0.6845371723175049, 0, 154, 93, 354], ['car', 0.39963772892951965, 78, 225, 113, 300], ['car', 0.22634947299957275, 421, 225, 483, 268], ['car', 0.21756017208099365, 421, 229, 470, 320]], 0.12596629999995912)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_runtime_image = run_inference_onnx_runtime_image()\n",
    "print(gpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 128.9ms\n",
      "Speed: 2.0ms preprocess, 128.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6303 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 118.5ms\n",
      "Speed: 1.0ms preprocess, 118.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6031 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 118.9ms\n",
      "Speed: 1.0ms preprocess, 118.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5954 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 121.8ms\n",
      "Speed: 2.0ms preprocess, 121.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6017 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 121.9ms\n",
      "Speed: 2.0ms preprocess, 121.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5878 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 121.6ms\n",
      "Speed: 1.5ms preprocess, 121.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5950 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 115.5ms\n",
      "Speed: 1.5ms preprocess, 115.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5890 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 114.9ms\n",
      "Speed: 0.5ms preprocess, 114.9ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5864 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 115.6ms\n",
      "Speed: 1.0ms preprocess, 115.6ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5795 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 122.1ms\n",
      "Speed: 2.0ms preprocess, 122.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.5929 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 126.0ms\n",
      "Speed: 2.0ms preprocess, 126.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6762 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 122.0ms\n",
      "Speed: 1.7ms preprocess, 122.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6697 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 119.4ms\n",
      "Speed: 1.8ms preprocess, 119.4ms inference, 22.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6832 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 126.6ms\n",
      "Speed: 2.0ms preprocess, 126.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6676 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 159.4ms\n",
      "Speed: 1.5ms preprocess, 159.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.7091 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 123.6ms\n",
      "Speed: 1.5ms preprocess, 123.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6985 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 169.8ms\n",
      "Speed: 2.0ms preprocess, 169.8ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.7437 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 122.3ms\n",
      "Speed: 1.0ms preprocess, 122.3ms inference, 22.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6954 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 169.3ms\n",
      "Speed: 2.0ms preprocess, 169.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.6961 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 124.4ms\n",
      "Speed: 1.0ms preprocess, 124.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 1.8301 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1231 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1194 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1242 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1241 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1241 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1238 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1234 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1218 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1277 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1260 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "result_time.append(collect_execution_times(run_inference_pytorch_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_runtime_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average GPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 21.4ms\n",
      "Speed: 1.0ms preprocess, 21.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2331 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 24.5ms\n",
      "Speed: 1.0ms preprocess, 24.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2532 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 19.2ms\n",
      "Speed: 2.0ms preprocess, 19.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2455 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2305 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 18.5ms\n",
      "Speed: 2.0ms preprocess, 18.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2484 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 13.4ms\n",
      "Speed: 1.0ms preprocess, 13.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2318 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 18.5ms\n",
      "Speed: 2.0ms preprocess, 18.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2421 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 18.1ms\n",
      "Speed: 0.0ms preprocess, 18.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2568 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 9.9ms\n",
      "Speed: 1.0ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.2894 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 255, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.1899 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 120.1ms\n",
      "Speed: 2.5ms preprocess, 120.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5120 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 132.0ms\n",
      "Speed: 3.0ms preprocess, 132.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5418 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 130.4ms\n",
      "Speed: 3.5ms preprocess, 130.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5414 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 139.4ms\n",
      "Speed: 2.0ms preprocess, 139.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5651 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 130.5ms\n",
      "Speed: 2.5ms preprocess, 130.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5576 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 144.5ms\n",
      "Speed: 3.0ms preprocess, 144.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5772 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 124.6ms\n",
      "Speed: 2.2ms preprocess, 124.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5544 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5490 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 118.0ms\n",
      "Speed: 2.0ms preprocess, 118.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.5427 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app\\weights\\yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 129.4ms\n",
      "Speed: 3.0ms preprocess, 129.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 596, 339]\n",
      "Class: person, Confidence: 0.87, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 255, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.75, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 321, 266, 443]\n",
      "Class: car, Confidence: 0.53, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.36, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.30, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.8203 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1318 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1289 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1252 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1208 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1190 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1242 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1278 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1221 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1223 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [286, 210, 459, 351]\n",
      "Class: person, Confidence: 0.90, Box: [159, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [209, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.68, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.40, Box: [78, 225, 113, 300]\n",
      "Class: car, Confidence: 0.23, Box: [421, 225, 483, 268]\n",
      "Class: car, Confidence: 0.22, Box: [421, 229, 470, 320]\n",
      "Elapsed time: 0.1232 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "result_time.append(collect_execution_times(run_inference_pytorch_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_runtime_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time gpu (ms)</th>\n",
       "      <th>onnx+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx runtime time gpu (ms)</th>\n",
       "      <th>pytorch time cpu (ms)</th>\n",
       "      <th>onnx+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx runtime time cpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.207207</td>\n",
       "      <td>57.614675</td>\n",
       "      <td>12.453119</td>\n",
       "      <td>159.611016</td>\n",
       "      <td>170.695167</td>\n",
       "      <td>12.375435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.511571</td>\n",
       "      <td>8.750798</td>\n",
       "      <td>0.394869</td>\n",
       "      <td>1.393987</td>\n",
       "      <td>4.864752</td>\n",
       "      <td>0.222625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.991850</td>\n",
       "      <td>51.198330</td>\n",
       "      <td>11.902980</td>\n",
       "      <td>157.950910</td>\n",
       "      <td>166.757570</td>\n",
       "      <td>11.940090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.378225</td>\n",
       "      <td>55.167835</td>\n",
       "      <td>12.372670</td>\n",
       "      <td>159.396420</td>\n",
       "      <td>169.575065</td>\n",
       "      <td>12.392515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>26.005414</td>\n",
       "      <td>60.154324</td>\n",
       "      <td>12.921933</td>\n",
       "      <td>160.582403</td>\n",
       "      <td>175.231857</td>\n",
       "      <td>12.619314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>27.474952</td>\n",
       "      <td>71.092402</td>\n",
       "      <td>13.051277</td>\n",
       "      <td>161.805066</td>\n",
       "      <td>179.123083</td>\n",
       "      <td>12.694842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.944490</td>\n",
       "      <td>82.030480</td>\n",
       "      <td>13.180620</td>\n",
       "      <td>163.027730</td>\n",
       "      <td>183.014310</td>\n",
       "      <td>12.770370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time gpu (ms)  onnx+ultralytics time gpu (ms)  \\\n",
       "count              10.000000                        10.000000   \n",
       "mean               24.207207                        57.614675   \n",
       "std                 2.511571                         8.750798   \n",
       "min                18.991850                        51.198330   \n",
       "50%                24.378225                        55.167835   \n",
       "90%                26.005414                        60.154324   \n",
       "95%                27.474952                        71.092402   \n",
       "max                28.944490                        82.030480   \n",
       "\n",
       "       onnx runtime time gpu (ms)  pytorch time cpu (ms)  \\\n",
       "count                   10.000000              10.000000   \n",
       "mean                    12.453119             159.611016   \n",
       "std                      0.394869               1.393987   \n",
       "min                     11.902980             157.950910   \n",
       "50%                     12.372670             159.396420   \n",
       "90%                     12.921933             160.582403   \n",
       "95%                     13.051277             161.805066   \n",
       "max                     13.180620             163.027730   \n",
       "\n",
       "       onnx+ultralytics time cpu (ms)  onnx runtime time cpu (ms)  \n",
       "count                        10.000000                   10.000000  \n",
       "mean                        170.695167                   12.375435  \n",
       "std                           4.864752                    0.222625  \n",
       "min                         166.757570                   11.940090  \n",
       "50%                         169.575065                   12.392515  \n",
       "90%                         175.231857                   12.619314  \n",
       "95%                         179.123083                   12.694842  \n",
       "max                         183.014310                   12.770370  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), \n",
    "                  columns=[\"pytorch time gpu (ms)\", \n",
    "                           \"onnx+ultralytics time gpu (ms)\", \n",
    "                           \"onnx runtime time gpu (ms)\",\n",
    "                           \"pytorch time cpu (ms)\", \n",
    "                           \"onnx+ultralytics time cpu (ms)\", \n",
    "                           \"onnx runtime time cpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_runtime_video = run_inference_onnx_video()\n",
    "print(onnx_runtime_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pt-onnx  cpu conf_diff          cpu box_diff pt-onnxruntime  cpu conf_diff            cpu box_diff\n",
      "0       car       0.000597  [0.0, 0.0, 0.0, 0.0]            car       0.006632    [2.0, 0.0, 2.0, 0.0]\n",
      "1       car       0.000376  [0.0, 0.0, 0.0, 0.0]            car       0.010218    [0.0, 0.0, 1.0, 1.0]\n",
      "2       car       0.000372  [0.0, 0.0, 0.0, 0.0]            car       0.005248    [2.0, 0.0, 1.0, 1.0]\n",
      "3    person       0.000532  [0.0, 0.0, 0.0, 0.0]         person       0.029649    [0.0, 0.0, 2.0, 0.0]\n",
      "4     truck       0.000260  [0.0, 1.0, 0.0, 0.0]          truck       0.028645    [1.0, 1.0, 2.0, 2.0]\n",
      "5     truck       0.014707  [0.0, 0.0, 0.0, 0.0]          truck       0.088956    [1.0, 1.0, 0.0, 1.0]\n",
      "6     truck       0.000880  [0.0, 0.0, 0.0, 0.0]          truck       0.062691    [0.0, 0.0, 1.0, 0.0]\n",
      "7   bicycle       0.000333  [0.0, 0.0, 0.0, 0.0]        bicycle       0.073267    [1.0, 1.0, 3.0, 2.0]\n",
      "8       car       0.006547  [0.0, 0.0, 0.0, 0.0]            car       0.119009   [0.0, 13.0, 0.0, 0.0]\n",
      "9       car       0.002422  [0.0, 0.0, 0.0, 0.0]            car       0.143279    [1.0, 3.0, 4.0, 1.0]\n",
      "10      car       0.001994  [0.0, 0.0, 0.0, 0.0]            car       0.073885  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"cpu conf_diff\", \"cpu box_diff\"])\n",
    "\n",
    "df_pt_onnx = generate_difference_df(pytorch_image[0], onnx_image[0], \"pt-onnx\")\n",
    "df_pt_onnxruntime = generate_difference_df(pytorch_image[0], onnx_runtime_image[0], \"pt-onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pt-onnx  gpu conf_diff          gpu box_diff pt-onnxruntime  gpu conf_diff            gpu box_diff\n",
      "0       car       0.000628  [0.0, 0.0, 0.0, 0.0]            car       0.006601    [2.0, 0.0, 2.0, 0.0]\n",
      "1       car       0.000427  [0.0, 0.0, 0.0, 0.0]            car       0.010167    [0.0, 0.0, 1.0, 1.0]\n",
      "2       car       0.000320  [0.0, 0.0, 0.0, 0.0]            car       0.005196    [2.0, 0.0, 1.0, 1.0]\n",
      "3    person       0.000423  [0.0, 0.0, 0.0, 0.0]         person       0.029541    [0.0, 0.0, 2.0, 0.0]\n",
      "4     truck       0.000275  [0.0, 1.0, 0.0, 0.0]          truck       0.028630    [1.0, 1.0, 2.0, 2.0]\n",
      "5     truck       0.014521  [0.0, 0.0, 0.0, 0.0]          truck       0.088770    [1.0, 1.0, 0.0, 1.0]\n",
      "6     truck       0.001256  [0.0, 0.0, 0.0, 0.0]          truck       0.063068    [0.0, 0.0, 1.0, 0.0]\n",
      "7   bicycle       0.000578  [0.0, 0.0, 0.0, 0.0]        bicycle       0.073022    [1.0, 1.0, 3.0, 2.0]\n",
      "8       car       0.006399  [0.0, 0.0, 0.0, 0.0]            car       0.119157   [0.0, 13.0, 0.0, 0.0]\n",
      "9       car       0.002188  [0.0, 0.0, 0.0, 0.0]            car       0.143513    [1.0, 3.0, 4.0, 1.0]\n",
      "10      car       0.001610  [0.0, 0.0, 0.0, 0.0]            car       0.073501  [1.0, 2.0, 19.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"gpu conf_diff\", \"gpu box_diff\"])\n",
    "\n",
    "df_pt_onnx = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_image[0], \"pt-onnx\")\n",
    "df_pt_onnxruntime = generate_difference_df(gpu_pytorch_image[0], gpu_onnx_runtime_image[0], \"pt-onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

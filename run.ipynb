{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timer import Timer\n",
    "from app.util.util import DotDict, Differ\n",
    "from main import yolo_run_image, yolo_run_video, inference_on_image, inference_on_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_pytorch_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_onnx_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})\n",
    "\n",
    "args_onnx_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"cpu\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pytorch_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"gpu\"\n",
    "})\n",
    "\n",
    "args_pytorch_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.pt\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"gpu\"\n",
    "})\n",
    "\n",
    "args_onnx_image = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_image.jpeg\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"image\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"gpu\"\n",
    "})\n",
    "\n",
    "args_onnx_video = DotDict({\n",
    "    \"weights\": \"./app/weights/yolov9c.onnx\", \n",
    "    \"source\": \"./app/assets/sample_video_2.mp4\", \n",
    "    \"classes\": \"./app/weights/metadata.yaml\", \n",
    "    \"type\": \"video\", \n",
    "    \"show\": False, \n",
    "    \"score_threshold\": 0.1, \n",
    "    \"conf_threshold\": 0.2, \n",
    "    \"iou_threshold\": 0.6, \n",
    "    \"device\": \"gpu\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pytorch_image():\n",
    "    result = yolo_run_image(args_pytorch_image)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_image():\n",
    "    result = yolo_run_image(args_onnx_image)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_runtime_image():\n",
    "    result = inference_on_image(args_onnx_image)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pytorch_video():\n",
    "    result = yolo_run_video(args_pytorch_video)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_video():\n",
    "    result = yolo_run_video(args_onnx_video)\n",
    "    return result\n",
    "\n",
    "def run_inference_onnx_runtime_video():\n",
    "    result = inference_on_video(args_onnx_video)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 144.7ms\n",
      "Speed: 1.8ms preprocess, 144.7ms inference, 277.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.6037 seconds\n",
      "([['car', 0.9309216141700745, 558, 206, 808, 359], ['car', 0.9193657040596008, 286, 210, 458, 352], ['car', 0.9118950366973877, 465, 217, 595, 340], ['person', 0.8777787089347839, 159, 143, 301, 403], ['truck', 0.8699429035186768, 103, 90, 256, 316], ['truck', 0.7752426862716675, 722, 170, 871, 346], ['truck', 0.7567529678344727, 0, 154, 94, 354], ['bicycle', 0.6531709432601929, 210, 320, 270, 442], ['car', 0.5153071880340576, 78, 212, 112, 300], ['car', 0.35083839297294617, 420, 226, 474, 319], ['car', 0.28913238644599915, 420, 227, 464, 278]], 0.6036842500000006)\n"
     ]
    }
   ],
   "source": [
    "pytorch_image = run_inference_pytorch_image()\n",
    "print(pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 188.7ms\n",
      "Speed: 2.1ms preprocess, 188.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3557 seconds\n",
      "([['car', 0.931485652923584, 558, 206, 808, 359], ['car', 0.9197295904159546, 286, 210, 458, 352], ['car', 0.9121138453483582, 465, 217, 595, 340], ['person', 0.8783342838287354, 159, 142, 301, 403], ['truck', 0.8705843687057495, 103, 89, 256, 316], ['truck', 0.790584921836853, 722, 170, 871, 346], ['truck', 0.7567024230957031, 0, 154, 94, 354], ['bicycle', 0.6545959115028381, 210, 320, 270, 442], ['car', 0.5217729210853577, 78, 212, 113, 300], ['car', 0.35309964418411255, 420, 226, 474, 319], ['car', 0.286945641040802, 420, 227, 464, 278]], 0.35571275000000036)\n"
     ]
    }
   ],
   "source": [
    "onnx_image = run_inference_onnx_image()\n",
    "print(onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1995 seconds\n",
      "([['car', 0.9240766763687134, 556, 206, 810, 359], ['car', 0.9173446297645569, 463, 217, 595, 338], ['car', 0.9110313653945923, 285, 210, 458, 351], ['person', 0.8804048299789429, 158, 143, 299, 403], ['truck', 0.8678995370864868, 723, 171, 871, 345], ['truck', 0.8387715816497803, 102, 89, 257, 314], ['bicycle', 0.7220539450645447, 208, 322, 269, 441], ['truck', 0.7081514596939087, 0, 154, 93, 354], ['car', 0.48693835735321045, 79, 226, 113, 300], ['car', 0.23660236597061157, 421, 229, 470, 320], ['car', 0.21792417764663696, 421, 224, 482, 267]], 0.199456584)\n"
     ]
    }
   ],
   "source": [
    "onnx_runtime_image = run_inference_onnx_runtime_image()\n",
    "print(onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 169.9ms\n",
      "Speed: 1.8ms preprocess, 169.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4076 seconds\n",
      "([['car', 0.9309216141700745, 558, 206, 808, 359], ['car', 0.9193657040596008, 286, 210, 458, 352], ['car', 0.9118950366973877, 465, 217, 595, 340], ['person', 0.8777787089347839, 159, 143, 301, 403], ['truck', 0.8699429035186768, 103, 90, 256, 316], ['truck', 0.7752426862716675, 722, 170, 871, 346], ['truck', 0.7567529678344727, 0, 154, 94, 354], ['bicycle', 0.6531709432601929, 210, 320, 270, 442], ['car', 0.5153071880340576, 78, 212, 112, 300], ['car', 0.35083839297294617, 420, 226, 474, 319], ['car', 0.28913238644599915, 420, 227, 464, 278]], 0.40755258399985905)\n"
     ]
    }
   ],
   "source": [
    "gpu_pytorch_image = run_inference_pytorch_image()\n",
    "print(gpu_pytorch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 197.3ms\n",
      "Speed: 1.9ms preprocess, 197.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3843 seconds\n",
      "([['car', 0.931485652923584, 558, 206, 808, 359], ['car', 0.9197295904159546, 286, 210, 458, 352], ['car', 0.9121138453483582, 465, 217, 595, 340], ['person', 0.8783342838287354, 159, 142, 301, 403], ['truck', 0.8705843687057495, 103, 89, 256, 316], ['truck', 0.790584921836853, 722, 170, 871, 346], ['truck', 0.7567024230957031, 0, 154, 94, 354], ['bicycle', 0.6545959115028381, 210, 320, 270, 442], ['car', 0.5217729210853577, 78, 212, 113, 300], ['car', 0.35309964418411255, 420, 226, 474, 319], ['car', 0.286945641040802, 420, 227, 464, 278]], 0.38425474999985454)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_image = run_inference_onnx_image()\n",
    "print(gpu_onnx_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yoloenv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1881 seconds\n",
      "([['car', 0.9240766763687134, 556, 206, 810, 359], ['car', 0.9173446297645569, 463, 217, 595, 338], ['car', 0.9110313653945923, 285, 210, 458, 351], ['person', 0.8804048299789429, 158, 143, 299, 403], ['truck', 0.8678995370864868, 723, 171, 871, 345], ['truck', 0.8387715816497803, 102, 89, 257, 314], ['bicycle', 0.7220539450645447, 208, 322, 269, 441], ['truck', 0.7081514596939087, 0, 154, 93, 354], ['car', 0.48693835735321045, 79, 226, 113, 300], ['car', 0.23660236597061157, 421, 229, 470, 320], ['car', 0.21792417764663696, 421, 224, 482, 267]], 0.1880799579998893)\n"
     ]
    }
   ],
   "source": [
    "gpu_onnx_runtime_image = run_inference_onnx_runtime_image()\n",
    "print(gpu_onnx_runtime_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 164.2ms\n",
      "Speed: 1.8ms preprocess, 164.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3905 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 162.7ms\n",
      "Speed: 1.5ms preprocess, 162.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3755 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 190.7ms\n",
      "Speed: 2.6ms preprocess, 190.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4261 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 175.8ms\n",
      "Speed: 1.6ms preprocess, 175.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3884 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 161.3ms\n",
      "Speed: 1.7ms preprocess, 161.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3544 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 168.1ms\n",
      "Speed: 1.2ms preprocess, 168.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3572 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 158.1ms\n",
      "Speed: 1.5ms preprocess, 158.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3589 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 185.8ms\n",
      "Speed: 1.4ms preprocess, 185.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3907 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 173.6ms\n",
      "Speed: 1.5ms preprocess, 173.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3738 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 156.6ms\n",
      "Speed: 1.3ms preprocess, 156.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3592 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 194.3ms\n",
      "Speed: 2.0ms preprocess, 194.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3855 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 188.6ms\n",
      "Speed: 1.7ms preprocess, 188.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3694 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 197.8ms\n",
      "Speed: 2.2ms preprocess, 197.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3840 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 192.4ms\n",
      "Speed: 2.0ms preprocess, 192.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3802 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 191.1ms\n",
      "Speed: 1.5ms preprocess, 191.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3813 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 191.2ms\n",
      "Speed: 1.7ms preprocess, 191.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3759 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 195.7ms\n",
      "Speed: 2.6ms preprocess, 195.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3771 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 190.7ms\n",
      "Speed: 2.2ms preprocess, 190.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3951 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 192.4ms\n",
      "Speed: 1.6ms preprocess, 192.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3796 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 191.3ms\n",
      "Speed: 1.6ms preprocess, 191.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4381 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1924 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1807 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1805 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1966 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1805 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1877 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1791 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1791 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1853 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1873 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "result_time.append(collect_execution_times(run_inference_pytorch_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_runtime_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average GPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 156.1ms\n",
      "Speed: 1.2ms preprocess, 156.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3665 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 200.0ms\n",
      "Speed: 4.6ms preprocess, 200.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4427 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 180.6ms\n",
      "Speed: 1.8ms preprocess, 180.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4103 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 178.9ms\n",
      "Speed: 2.3ms preprocess, 178.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4082 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 166.9ms\n",
      "Speed: 1.5ms preprocess, 166.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3714 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 163.5ms\n",
      "Speed: 1.2ms preprocess, 163.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3654 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 160.4ms\n",
      "Speed: 1.5ms preprocess, 160.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3650 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 161.2ms\n",
      "Speed: 1.7ms preprocess, 161.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3638 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 164.3ms\n",
      "Speed: 1.6ms preprocess, 164.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3755 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 6 cars, 3 trucks, 157.9ms\n",
      "Speed: 1.6ms preprocess, 157.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 143, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 90, 256, 316]\n",
      "Class: truck, Confidence: 0.78, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 112, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3557 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 187.8ms\n",
      "Speed: 1.8ms preprocess, 187.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3760 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 187.7ms\n",
      "Speed: 1.8ms preprocess, 187.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3702 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 188.9ms\n",
      "Speed: 2.0ms preprocess, 188.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3697 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 191.4ms\n",
      "Speed: 1.4ms preprocess, 191.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3849 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 192.7ms\n",
      "Speed: 2.6ms preprocess, 192.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3803 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 188.7ms\n",
      "Speed: 2.1ms preprocess, 188.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3686 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 189.2ms\n",
      "Speed: 2.0ms preprocess, 189.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3706 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 192.6ms\n",
      "Speed: 1.3ms preprocess, 192.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3807 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 188.9ms\n",
      "Speed: 1.4ms preprocess, 188.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.3779 seconds\n",
      "[INFO] Initialize Model\n",
      "[INFO] Inference Image\n",
      "Loading app/weights/yolov9c.onnx for ONNX Runtime inference...\n",
      "\n",
      "0: 640x640 1 person, 1 bicycle, 6 cars, 3 trucks, 189.8ms\n",
      "Speed: 2.5ms preprocess, 189.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Class: car, Confidence: 0.93, Box: [558, 206, 808, 359]\n",
      "Class: car, Confidence: 0.92, Box: [286, 210, 458, 352]\n",
      "Class: car, Confidence: 0.91, Box: [465, 217, 595, 340]\n",
      "Class: person, Confidence: 0.88, Box: [159, 142, 301, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [103, 89, 256, 316]\n",
      "Class: truck, Confidence: 0.79, Box: [722, 170, 871, 346]\n",
      "Class: truck, Confidence: 0.76, Box: [0, 154, 94, 354]\n",
      "Class: bicycle, Confidence: 0.65, Box: [210, 320, 270, 442]\n",
      "Class: car, Confidence: 0.52, Box: [78, 212, 113, 300]\n",
      "Class: car, Confidence: 0.35, Box: [420, 226, 474, 319]\n",
      "Class: car, Confidence: 0.29, Box: [420, 227, 464, 278]\n",
      "Elapsed time: 0.4255 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1916 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1801 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1819 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1795 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1804 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1794 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1868 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1806 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1855 seconds\n",
      "[INFO] Intialize Model\n",
      "[INFO] Inference Image\n",
      "Class: car, Confidence: 0.92, Box: [556, 206, 810, 359]\n",
      "Class: car, Confidence: 0.92, Box: [463, 217, 595, 338]\n",
      "Class: car, Confidence: 0.91, Box: [285, 210, 458, 351]\n",
      "Class: person, Confidence: 0.88, Box: [158, 143, 299, 403]\n",
      "Class: truck, Confidence: 0.87, Box: [723, 171, 871, 345]\n",
      "Class: truck, Confidence: 0.84, Box: [102, 89, 257, 314]\n",
      "Class: bicycle, Confidence: 0.72, Box: [208, 322, 269, 441]\n",
      "Class: truck, Confidence: 0.71, Box: [0, 154, 93, 354]\n",
      "Class: car, Confidence: 0.49, Box: [79, 226, 113, 300]\n",
      "Class: car, Confidence: 0.24, Box: [421, 229, 470, 320]\n",
      "Class: car, Confidence: 0.22, Box: [421, 224, 482, 267]\n",
      "Elapsed time: 0.1794 seconds\n"
     ]
    }
   ],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "result_time.append(collect_execution_times(run_inference_pytorch_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_image))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_runtime_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch time cpu (ms)</th>\n",
       "      <th>onnx+ultralytics time cpu (ms)</th>\n",
       "      <th>onnx runtime time cpu (ms)</th>\n",
       "      <th>pytorch time gpu (ms)</th>\n",
       "      <th>onnx+ultralytics time gpu (ms)</th>\n",
       "      <th>onnx runtime time gpu (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.746770</td>\n",
       "      <td>38.661681</td>\n",
       "      <td>18.492486</td>\n",
       "      <td>38.245286</td>\n",
       "      <td>38.044466</td>\n",
       "      <td>18.252970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.228802</td>\n",
       "      <td>1.928722</td>\n",
       "      <td>0.606602</td>\n",
       "      <td>2.820023</td>\n",
       "      <td>1.677831</td>\n",
       "      <td>0.410607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>35.438800</td>\n",
       "      <td>36.943508</td>\n",
       "      <td>17.908117</td>\n",
       "      <td>35.570558</td>\n",
       "      <td>36.861346</td>\n",
       "      <td>17.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.464694</td>\n",
       "      <td>38.074748</td>\n",
       "      <td>18.300533</td>\n",
       "      <td>36.895510</td>\n",
       "      <td>37.695465</td>\n",
       "      <td>18.052623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>39.423612</td>\n",
       "      <td>39.942770</td>\n",
       "      <td>19.282160</td>\n",
       "      <td>41.355751</td>\n",
       "      <td>38.896257</td>\n",
       "      <td>18.723857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>41.019294</td>\n",
       "      <td>41.874654</td>\n",
       "      <td>19.471896</td>\n",
       "      <td>42.815198</td>\n",
       "      <td>40.723380</td>\n",
       "      <td>18.941199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.614975</td>\n",
       "      <td>43.806538</td>\n",
       "      <td>19.661633</td>\n",
       "      <td>44.274646</td>\n",
       "      <td>42.550504</td>\n",
       "      <td>19.158542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pytorch time cpu (ms)  onnx+ultralytics time cpu (ms)  onnx runtime time cpu (ms)  pytorch time gpu (ms)  onnx+ultralytics time gpu (ms)  onnx runtime time gpu (ms)\n",
       "count              10.000000                        10.000000                   10.000000              10.000000                        10.000000                   10.000000\n",
       "mean               37.746770                        38.661681                   18.492486              38.245286                        38.044466                   18.252970\n",
       "std                 2.228802                         1.928722                    0.606602               2.820023                         1.677831                    0.410607\n",
       "min                35.438800                        36.943508                   17.908117              35.570558                        36.861346                   17.935800\n",
       "50%                37.464694                        38.074748                   18.300533              36.895510                        37.695465                   18.052623\n",
       "90%                39.423612                        39.942770                   19.282160              41.355751                        38.896257                   18.723857\n",
       "95%                41.019294                        41.874654                   19.471896              42.815198                        40.723380                   18.941199\n",
       "max                42.614975                        43.806538                   19.661633              44.274646                        42.550504                   19.158542"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time = np.array(result_time)\n",
    "df = pd.DataFrame(np.transpose(result_time), columns=[\"pytorch time cpu (ms)\", \"onnx+ultralytics time cpu (ms)\", \"onnx runtime time cpu (ms)\", \"pytorch time gpu (ms)\", \"onnx+ultralytics time gpu (ms)\", \"onnx runtime time gpu (ms)\"])\n",
    "df.describe(percentiles=[.9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CPU Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_execution_times(run_inference_func, iterations=10):\n",
    "    execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        execution_time = run_inference_func()\n",
    "        execution_times.append(execution_time[1] * 100)\n",
    "    return execution_times\n",
    "\n",
    "result_time.append(collect_execution_times(run_inference_pytorch_video))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_video))\n",
    "result_time.append(collect_execution_times(run_inference_onnx_runtime_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pt-onnx  conf_diff              box_diff pt-onnxruntime  conf_diff                box_diff\n",
      "0       car   0.000564  [0.0, 0.0, 0.0, 0.0]            car   0.006845    [2.0, 0.0, 2.0, 0.0]\n",
      "1       car   0.000364  [0.0, 0.0, 0.0, 0.0]            car   0.008334    [1.0, 0.0, 0.0, 1.0]\n",
      "2       car   0.000219  [0.0, 0.0, 0.0, 0.0]            car   0.005450    [2.0, 0.0, 0.0, 2.0]\n",
      "3    person   0.000556  [0.0, 1.0, 0.0, 0.0]         person   0.002626    [1.0, 0.0, 2.0, 0.0]\n",
      "4     truck   0.000641  [0.0, 1.0, 0.0, 0.0]          truck   0.031171    [1.0, 1.0, 1.0, 2.0]\n",
      "5     truck   0.015342  [0.0, 0.0, 0.0, 0.0]          truck   0.092657    [1.0, 1.0, 0.0, 1.0]\n",
      "6     truck   0.000051  [0.0, 0.0, 0.0, 0.0]          truck   0.048602    [0.0, 0.0, 1.0, 0.0]\n",
      "7   bicycle   0.001425  [0.0, 0.0, 0.0, 0.0]        bicycle   0.068883    [2.0, 2.0, 1.0, 1.0]\n",
      "8       car   0.006466  [0.0, 0.0, 1.0, 0.0]            car   0.028369   [1.0, 14.0, 1.0, 0.0]\n",
      "9       car   0.002261  [0.0, 0.0, 0.0, 0.0]            car   0.114236    [1.0, 3.0, 4.0, 1.0]\n",
      "10      car   0.002187  [0.0, 0.0, 0.0, 0.0]            car   0.071208  [1.0, 3.0, 18.0, 11.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"conf_diff\", \"box_diff\"])\n",
    "\n",
    "df_pt_onnx = generate_difference_df(pytorch_image[0], onnx_image[0], \"pt-onnx\")\n",
    "df_pt_onnxruntime = generate_difference_df(pytorch_image[0], onnx_runtime_image[0], \"pt-onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pt-onnx  conf_diff              box_diff pt-onnxruntime  conf_diff                box_diff\n",
      "0       car   0.000564  [0.0, 0.0, 0.0, 0.0]            car   0.006845    [2.0, 0.0, 2.0, 0.0]\n",
      "1       car   0.000364  [0.0, 0.0, 0.0, 0.0]            car   0.008334    [1.0, 0.0, 0.0, 1.0]\n",
      "2       car   0.000219  [0.0, 0.0, 0.0, 0.0]            car   0.005450    [2.0, 0.0, 0.0, 2.0]\n",
      "3    person   0.000556  [0.0, 1.0, 0.0, 0.0]         person   0.002626    [1.0, 0.0, 2.0, 0.0]\n",
      "4     truck   0.000641  [0.0, 1.0, 0.0, 0.0]          truck   0.031171    [1.0, 1.0, 1.0, 2.0]\n",
      "5     truck   0.015342  [0.0, 0.0, 0.0, 0.0]          truck   0.092657    [1.0, 1.0, 0.0, 1.0]\n",
      "6     truck   0.000051  [0.0, 0.0, 0.0, 0.0]          truck   0.048602    [0.0, 0.0, 1.0, 0.0]\n",
      "7   bicycle   0.001425  [0.0, 0.0, 0.0, 0.0]        bicycle   0.068883    [2.0, 2.0, 1.0, 1.0]\n",
      "8       car   0.006466  [0.0, 0.0, 1.0, 0.0]            car   0.028369   [1.0, 14.0, 1.0, 0.0]\n",
      "9       car   0.002261  [0.0, 0.0, 0.0, 0.0]            car   0.114236    [1.0, 3.0, 4.0, 1.0]\n",
      "10      car   0.002187  [0.0, 0.0, 0.0, 0.0]            car   0.071208  [1.0, 3.0, 18.0, 11.0]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def generate_difference_df(image1, image2, label):\n",
    "    differ = Differ(np.array(image1), np.array(image2))\n",
    "    result = differ.find_difference()\n",
    "    return pd.DataFrame(result, columns=[label, \"conf_diff\", \"box_diff\"])\n",
    "\n",
    "df_pt_onnx = generate_difference_df(pytorch_image[0], onnx_image[0], \"pt-onnx\")\n",
    "df_pt_onnxruntime = generate_difference_df(pytorch_image[0], onnx_runtime_image[0], \"pt-onnxruntime\")\n",
    "\n",
    "df_combined = pd.concat([df_pt_onnx, df_pt_onnxruntime], axis=1)\n",
    "\n",
    "print(df_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
